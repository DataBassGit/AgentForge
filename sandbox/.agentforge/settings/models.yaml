# Default model settings for all agents unless overridden
default_model:
  api: gemini_api
  model: gemini_flash
# Uncomment to use alternative default models
#  api: lm_studio_api
#  model: LMStudio
#  api: openai_api
#  model: omni_model
#  model: o1_preview

# Library of models and parameter defaults
model_library:
  openai_api:  # API script name
    O1Series:  # Class name for the API (Case Sensitive)
      models:  # List of model configurations
        o1:
          identifier: o1
          params: {}  # No overrides for this model
        o1_preview:
          identifier: o1-preview
        o1_mini:
          identifier: o1-mini

      params: {}  # Default parameters for the model class

    GPT:
      models:
        omni_model:
          identifier: gpt-4o
          params:
            max_new_tokens: 15000  # Example of overriding parameters
        smart_model:
          identifier: gpt-4
        smart_fast_model:
          identifier: gpt-4-turbo
        fast_model:
          identifier: gpt-3.5-turbo

      params:  # Default parameters for GPT models
        max_tokens: 10000
        n: 1
        presence_penalty: 0
        stop: null
        temperature: 0.8
        top_p: 0.1

  anthropic_api:
    Claude:
      models:
        claude3:
          identifier: claude-3-opus-20240229

      params:  # Default parameters for Claude models
        max_tokens: 10000
        temperature: 0.8
        top_p: 0.1

  gemini_api:
    Gemini:
      models:
        gemini_pro:
          identifier: gemini-1.5-pro
        gemini_flash:
          identifier: gemini-1.5-flash

      params:  # Default parameters for Gemini models
        candidate_count: 1
        max_output_tokens: 10000
        temperature: 0.8
        top_k: 40
        top_p: 0.1

  lm_studio_api:
    LMStudio:
      models:
        llama3_8b:
          identifier: lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF

      params:  # Default parameters for LMStudio models
        host_url: http://localhost:1234/v1/chat/completions
        max_tokens: 10000
        stream: false
        temperature: 0.8

  ollama_api:
    Ollama:
      models:
        llama3.1_70b:
          identifier: llama3.1:70b

      params:  # Default parameters for Ollama models
        host_url: http://localhost:11434/api/generate
        max_tokens: 10000
        stream: false
        temperature: 0.8

  openrouter_api:
    OpenRouter:
      models:
        phi3med:
          identifier: microsoft/phi-3-medium-128k-instruct:free
        hermes:
          identifier: nousresearch/hermes-3-llama-3.1-405b
        reflection:
          identifier: mattshumer/reflection-70b:free

  groq_api:
    GroqAPI:
      models:
        llama31:
          identifier: llama-3.1-70b-versatile

      params:  # Default parameters for GroqAPI models
        max_tokens: 10000
        seed: -1
        stop: null
        temperature: 0.8
        top_p: 0.1

# Embedding library
embedding_library:
  library: sentence_transformers
