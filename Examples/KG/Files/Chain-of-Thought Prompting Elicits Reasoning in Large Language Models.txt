Chain-of-Thought Prompting Elicits Reasoning
in Large Language Models
Jason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma
Brian Ichter Fei Xia Ed H. Chi Quoc V . Le Denny Zhou
Google Research, Brain Team
{jasonwei,dennyzhou}@google.com
Abstract
We explore how generating a chain of thought —a series of intermediate reasoning
steps—signiﬁcantly improves the ability of large language models to perform
complex reasoning. In particular, we show how such reasoning abilities emerge
naturally in sufﬁciently large language models via a simple method called chain-of-
thought prompting , where a few chain of thought demonstrations are provided as
exemplars in prompting.
Experiments on three large language models show that chain-of-thought prompting
improves performance on a range of arithmetic, commonsense, and symbolic
reasoning tasks. The empirical gains can be striking. For instance, prompting a
PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art
accuracy on the GSM8K benchmark of math word problems, surpassing even
ﬁnetuned GPT-3 with a veriﬁer.
A: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9.Chain-of-Thought PromptingQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?A: The answer is 27.Standard Prompting
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?Model Input
Model OutputModel OutputModel Input
Figure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic,
commonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted.
36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2201.11903v6  [cs.CL]  10 Jan 20231 Introduction
Math Word Problems (GSM8K)020406080100
3355
1857Solve rate (%)Finetuned GPT-3 175B
Prior best
PaLM 540B: standard prompting
PaLM 540B: chain-of-thought prompting
Figure 2: PaLM 540B uses chain-of-
thought prompting to achieve new state-
of-the-art performance on the GSM8K
benchmark of math word problems.
Finetuned GPT-3 and prior best are from
Cobbe et al. (2021).The NLP landscape has recently been revolutionized by
language models (Peters et al., 2018; Devlin et al., 2019;
Brown et al., 2020, inter alia ). Scaling up the size of lan-
guage models has been shown to confer a range of beneﬁts,
such as improved performance and sample efﬁciency (Ka-
plan et al., 2020; Brown et al., 2020, inter alia ). However,
scaling up model size alone has not proved sufﬁcient for
achieving high performance on challenging tasks such as
arithmetic, commonsense, and symbolic reasoning (Rae
et al., 2021).
This work explores how the reasoning ability of large
language models can be unlocked by a simple method
motivated by two ideas. First, techniques for arithmetic
reasoning can beneﬁt from generating natural language
rationales that lead to the ﬁnal answer. Prior work has
given models the ability to generate natural language inter-
mediate steps by training from scratch (Ling et al., 2017)
or ﬁnetuning a pretrained model (Cobbe et al., 2021), in
addition to neuro-symbolic methods that use formal lan-
guages instead of natural language (Roy and Roth, 2015;
Chiang and Chen, 2019; Amini et al., 2019; Chen et al.,
2019). Second, large language models offer the exciting
prospect of in-context few-shot learning via prompting . That is, instead of ﬁnetuning a separate
language model checkpoint for each new task, one can simply “prompt” the model with a few
input–output exemplars demonstrating the task. Remarkably, this has been successful for a range of
simple question-answering tasks (Brown et al., 2020).
Both of the above ideas, however, have key limitations. For rationale-augmented training and
ﬁnetuning methods, it is costly to create a large set of high quality rationales, which is much more
complicated than simple input–output pairs used in normal machine learning. For the traditional few-
shot prompting method used in Brown et al. (2020), it works poorly on tasks that require reasoning
abilities, and often does not improve substantially with increasing language model scale (Rae et al.,
2021). In this paper, we combine the strengths of these two ideas in a way that avoids their limitations.
Speciﬁcally, we explore the ability of language models to perform few-shot prompting for reasoning
tasks, given a prompt that consists of triples: hinput, chain of thought , outputi. Achain of thought is
a series of intermediate natural language reasoning steps that lead to the ﬁnal output, and we refer to
this approach as chain-of-thought prompting . An example prompt is shown in Figure 1.
We present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks,
showing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking
degree. Figure 2 illustrates one such result—on the GSM8K benchmark of math word problems
(Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting
by a large margin and achieves new state-of-the-art performance. A prompting only approach is
important because it does not require a large training dataset and because a single model checkpoint
can perform many tasks without loss of generality. This work underscores how large language models
can learn via a few examples with natural language data about the task (c.f. automatically learning
the patterns underlying inputs and outputs via a large training dataset).
2 Chain-of-Thought Prompting
Consider one’s own thought process when solving a complicated reasoning task such as a multi-step
math word problem. It is typical to decompose the problem into intermediate steps and solve each
before giving the ﬁnal answer: “After Jane gives 2 ﬂowers to her mom she has 10 :::then after she
gives 3 to her dad she will have 7 :::so the answer is 7. ” The goal of this paper is to endow language
models with the ability to generate a similar chain of thought —a coherent series of intermediate
reasoning steps that lead to the ﬁnal answer for a problem. We will show that sufﬁciently large
2language models can generate chains of thought if demonstrations of chain-of-thought reasoning are
provided in the exemplars for few-shot prompting.
Figure 1 shows an example of a model producing a chain of thought to solve a math word problem
that it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution
and can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it
mimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations
typically come after the ﬁnal answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al.,
2022, inter alia )).
Chain-of-thought prompting has several attractive properties as an approach for facilitating reasoning
in language models.
1.First, chain of thought, in principle, allows models to decompose multi-step problems into
intermediate steps, which means that additional computation can be allocated to problems
that require more reasoning steps.
2.Second, a chain of thought provides an interpretable window into the behavior of the model,
suggesting how it might have arrived at a particular answer and providing opportunities
to debug where the reasoning path went wrong (although fully characterizing a model’s
computations that support an answer remains an open question).
3.Third, chain-of-thought reasoning can be used for tasks such as math word problems,
commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least
in principle) to any task that humans can solve via language.
4.Finally, chain-of-thought reasoning can be readily elicited in sufﬁciently large off-the-shelf
language models simply by including examples of chain of thought sequences into the
exemplars of few-shot prompting.
In empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic
reasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5).
3 Arithmetic Reasoning
We begin by considering math word problems of the form in Figure 1, which measure the arithmetic
reasoning ability of language models. Though simple for humans, arithmetic reasoning is a task where
language models often struggle (Hendrycks et al., 2021; Patel et al., 2021, inter alia ). Strikingly, chain-
of-thought prompting when used with the 540B parameter language model performs comparably with
task-speciﬁc ﬁnetuned models on several tasks, even achieving new state of the art on the challenging
GSM8K benchmark (Cobbe et al., 2021).
3.1 Experimental Setup
We explore chain-of-thought prompting for various language models on multiple benchmarks.
Benchmarks. We consider the following ﬁve math word problem benchmarks: (1)theGSM8K
benchmark of math word problems (Cobbe et al., 2021), (2)theSV AMP dataset of math word
problems with varying structures (Patel et al., 2021), (3)theASDiv dataset of diverse math word
problems (Miao et al., 2020), (4)theAQuA dataset of algebraic word problems, and (5)theMA WPS
benchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.
Standard prompting. For the baseline, we consider standard few-shot prompting, popularized by
Brown et al. (2020), in which a language model is given in-context exemplars of input–output pairs
before outputting a prediction for a test-time example. Exemplars are formatted as questions and
answers. The model gives the answer directly, as shown in Figure 1 (left).
Chain-of-thought prompting. Our proposed approach is to augment each exemplar in few-shot
prompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most
of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars
with chains of thought for prompting—Figure 1 (right) shows one chain of thought exemplar, and the
full set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo
prompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether
chain-of-thought prompting in this form can successfully elicit successful reasoning across a range of
3Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.Q: Sammy wanted to go to where the people were. Where might he go? Options: (a) race track (b) populated areas (c) desert (d) apartment (e) roadblock A: The answer must be a place with a lot of people. Race tracks, desert, apartments, and roadblocks don't have a lot of people, but populated areas do. So the answer is (b). Q: Yes or no: Would a pear sink in water? A: The density of a pear is about 0.6 g/cm^3, which is less than water. Thus, a pear would float. So the answer is no.Q: The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY?  A: One day after 06/01/1943 is 06/02/1943, so today is 06/02/1943. 10 days before today is 05/23/1943. So the answer is 05/23/1943. Q: Is the following sentence plausible? "Joao Moutinho caught the screen pass in the NFC championship."  A: Joao Moutinho is a soccer player. The NFC championship is part of American football, not soccer. So the answer is no.Q: Take the last letters of the words in “Lady Gaga” and concatenate them. A: The last letter of “Lady” is “y”. The last letter of “Gaga” is “a”. Concatenating them is “ya”. So the answer is ya.Q: A coin is heads up. Maybelle flips the coin. Shalonda does not flip the coin. Is the coin still heads up? A: The coin was flipped by Maybelle. So the coin was flipped 1 time, which is an odd number. The coin started heads up, so after an odd number of flips, it will be tails up. So the answer is no.Math Word Problems (free response)Math Word Problems (multiple choice)CSQA (commonsense)
StrategyQADate UnderstandingSports Understanding
Last Letter ConcatenationCoin Flip (state tracking)Q: How many keystrokes are needed to type the numbers from 1 to 500?Answer Choices: (a) 1156 (b) 1392 (c) 1480 (d) 1562 (e) 1788 A: There are 9 one-digit numbers from 1 to 9. There are 90 two-digit numbers from 10 to 99. There are 401 three-digit numbers from 100 to 500. 9 + 90(2) + 401(3) = 1392. The answer is (b).
SayCan (Instructing a robot)Human: How would you bring me something that isn’t a fruit? Explanation: the user wants something to eat that isn’t a fruit. An energy bar is not a fruit, so I will bring the user an energy bar.  Plan: 1. find(energy bar) 2. pick(energy bar) 3. find(user) 4. put(energy bar) 5. done().Figure 3: Examples of hinput, chain of thought, output itriples for arithmetic, commonsense, and
symbolic reasoning benchmarks. Chains of thought are highlighted. Full prompts in Appendix G.
math word problems, we used this single set of eight chain of thought exemplars for all benchmarks
except AQuA, which is multiple choice instead of free response. For AQuA, we used four exemplars
and solutions from the training set, as given in Appendix Table 21.
Language models. We evaluate ﬁve large language models. The ﬁrst is GPT-3 (Brown et al.,
2020), for which we use text-ada-001, text-babbage-001, text-curie-001, and text-davinci-002, which
presumably correspond to InstructGPT models of 350M, 1.3B, 6.7B, and 175B parameters (Ouyang
et al., 2022).The second is LaMDA (Thoppilan et al., 2022), which has models of 422M, 2B, 8B,
68B, and 137B parameters. The third is PaLM , which has models of 8B, 62B, and 540B parameters.
The fourth is UL2 20B (Tay et al., 2022), and the ﬁfth is Codex (Chen et al., 2021, code-davinci-002
in the OpenAI API). We sample from the models via greedy decoding (though follow-up work shows
chain-of-thought prompting can be improved by taking the majority ﬁnal answer over many sampled
generations (Wang et al., 2022a)). For LaMDA, we report averaged results over ﬁve random seeds,
where each seed had a different randomly shufﬂed order of exemplars. As LaMDA experiments
did not show large variance among different seeds, to save compute we report results for a single
exemplar order for all other models.
3.2 Results
The strongest results of chain-of-thought prompting are summarized in Figure 4, with all experimental
outputs for each model collection, model size, and benchmark shown in Table 2 in the Appendix.
There are three key takeaways. First, Figure 4 shows that chain-of-thought prompting is an emergent
ability of model scale (Wei et al., 2022b). That is, chain-of-thought prompting does not positively
impact performance for small models, and only yields performance gains when used with models of
100B parameters. We qualitatively found that models of smaller scale produced ﬂuent but illogical
chains of thought, leading to lower performance than standard prompting.
40204060GSM8K
solve rate (%)LaMDA GPT PaLMStandard prompting
Chain-of-thought prompting
Prior supervised best
020406080SV AMP
solve rate (%)
0.4 81370255075100MAWPS
solve rate (%)
0.4 7175 862540
Model scale (# parameters in billions)
Figure 4: Chain-of-thought prompting enables
large language models to solve challenging math
problems. Notably, chain-of-thought reasoning
is an emergent ability of increasing model scale.
Prior best numbers are from Cobbe et al. (2021)
for GSM8K, Jie et al. (2022) for SV AMP, and Lan
et al. (2021) for MAWPS.Second, chain-of-thought prompting has larger
performance gains for more-complicated prob-
lems. For instance, for GSM8K (the dataset
with the lowest baseline performance), perfor-
mance more than doubled for the largest GPT
and PaLM models. On the other hand, for Sin-
gleOp, the easiest subset of MAWPS which only
requires a single step to solve, performance im-
provements were either negative or very small
(see Appendix Table 3).
Third, chain-of-thought prompting via GPT-3
175B and PaLM 540B compares favorably to
prior state of the art, which typically ﬁnetunes a
task-speciﬁc model on a labeled training dataset.
Figure 4 shows how PaLM 540B uses chain-of-
thought prompting to achieve new state of the art
on GSM8K, SV AMP, and MAWPS (though note
that standard prompting already passed the prior
best for SV AMP). On the other two datasets,
AQuA and ASDiv, PaLM with chain-of-thought
prompting reaches within 2% of the state of the
art (Appendix Table 2).
To better understand why chain-of-thought
prompting works, we manually examined model-
generated chains of thought by LaMDA 137B
for GSM8K. Of 50 random examples where the
model returned the correct ﬁnal answer, all of
the generated chains of thought were also log-
ically and mathematically correct except two
that coincidentally arrived at the correct answer
(see Appendix D.1, and Table 8 for examples
of correct model-generated chains of thought).
We also randomly examined 50 random sam-
ples for which the model gave the wrong answer.
The summary of this analysis is that 46% of the
chains of thought were almost correct, barring
minor mistakes (calculator error, symbol map-
ping error, or one reasoning step missing), and that the other 54% of the chains of thought had major
errors in semantic understanding or coherence (see Appendix D.2). To provide a small insight into
why scaling improves chain-of-thought reasoning ability, we performed a similar analysis of errors
made by PaLM 62B and whether those errors were ﬁxed by scaling to PaLM 540B. The summary
is that scaling PaLM to 540B ﬁxes a large portion of one-step missing and semantic understanding
errors in the 62B model (see Appendix A.1).
3.3 Ablation Study
The observed beneﬁts of using chain-of-thought prompting raises the natural question of whether the
same performance improvements can be conferred via other types of prompting. Figure 5 shows an
ablation study with three variations of chain of thought described below.
Equation only. One reason for why chain-of-thought prompting might help is that it produces the
mathematical equation to be evaluated, and so we test a variation where the model is prompted
to output only a mathematical equation before giving the answer. Figure 5 shows that equation
only prompting does not help much for GSM8K, which implies that the semantics of the questions
in GSM8K are too challenging to directly translate into an equation without the natural language
reasoning steps in chain of thought. For datasets of one-step or two-step problems, however, we ﬁnd
that equation only prompting does improve performance, since the equation can be easily derived
from the question (see Appendix Table 6).
5LaMDA PaLM0204060GSM8K solve rate (%)Standard prompting
Equation only
Variable compute only
Reasoning after answer
Chain-of-thought prompting
Figure 5: Ablation study for dif-
ferent variations of prompting us-
ing LaMDA 137B and PaLM 540B.
Results for other datasets are given
in Appendix Table 6 and Table 7.Variable compute only. Another intuition is that chain of
thought allows the model to spend more computation (i.e.,
intermediate tokens) on harder problems. To isolate the effect
of variable computation from chain-of-thought reasoning, we
test a conﬁguration where the model is prompted to output a
only sequence of dots ( :::) equal to the number of characters in
the equation needed to solve the problem. This variant performs
about the same as the baseline, which suggests that variable
computation by itself is not the reason for the success of chain-
of-thought prompting, and that there appears to be utility from
expressing intermediate steps via natural language.
Chain of thought after answer. Another potential beneﬁt of
chain-of-thought prompting could simply be that such prompts
allow the model to better access relevant knowledge acquired
during pretraining. Therefore, we test an alternative conﬁgura-
tion where the chain of thought prompt is only given after the
answer, isolating whether the model actually depends on the
produced chain of thought to give the ﬁnal answer. This variant
performs about the same as the baseline, which suggests that
the sequential reasoning embodied in the chain of thought is
useful for reasons beyond just activating knowledge.
3.4 Robustness of Chain of Thought
GSM8K05101520Solve rate (%)Standard prompting
Chain-of-thought prompting
different annotator (B)
different annotator (C)
intentionally concise style
exemplars from GSM8K ( )
exemplars from GSM8K ( )
exemplars from GSM8K ( )
MAWPS0204060
Figure 6: Chain-of-thought prompting
has variance for different prompt exam-
ples (as expected) but outperforms stan-
dard prompting for various annotators as
well as for different exemplars.Sensitivity to exemplars is a key consideration of prompt-
ing approaches—for instance, varying the permutation of
few-shot exemplars can cause the accuracy of GPT-3 on
SST-2 to range from near chance (54.3%) to near state of
the art (93.4%) (Zhao et al., 2021). In this ﬁnal subsec-
tion, we evaluate robustness to chains of thought written
by different annotators. In addition to the results above,
which used chains of thought written by an Annotator
A, two other co-authors of this paper (Annotators B and
C) independently wrote chains of thought for the same
few-shot exemplars (shown in Appendix H). Annotator A
also wrote another chain of thought that was more concise
than the original, following the style of solutions given in
Cobbe et al. (2021).1
Figure 6 shows these results for LaMDA 137B on GSM8K
and MAWPS (ablation results for other datasets are given
in Appendix Table 6 / Table 7). Although there is variance
among different chain of thought annotations, as would be
expected when using exemplar-based prompting (Le Scao
and Rush, 2021; Reynolds and McDonell, 2021; Zhao
et al., 2021), all sets of chain of thought prompts outper-
form the standard baseline by a large margin. This result
implies that successful use of chain of thought does not
depend on a particular linguistic style.
To conﬁrm that successful chain-of-thought prompting
works for other sets of exemplars, we also run experiments
with three sets of eight exemplars randomly sampled from the GSM8K training set, an independent
1For instance, whereas original chain of thought uses several short sentences ( “’There were originally 9
computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is
29. ”), the concise chain of thought would read “5 * 4 = 20 new computers were added. So there are 9 + 20 = 29
new computers in the server room now” .
6source (examples in this dataset already included reasoning steps like a chain of thought).2Fig-
ure 6 shows that these prompts performed comparably with our manually written exemplars, also
substantially outperforming standard prompting.
In addition to robustness to annotators, independently-written chains of thought, different exemplars,
and various language models, we also ﬁnd that chain-of-thought prompting for arithmetic reasoning
is robust to different exemplar orders and varying numbers of exemplars (see Appendix A.2).
4 Commonsense Reasoning
Although chain of thought is particularly suitable for math word problems, the language-based nature
of chain of thought actually makes it applicable to a broad class of commonsense reasoning problems,
which involve reasoning about physical and human interactions under the presumption of general
background knowledge. Commonsense reasoning is key for interacting with the world and is still
beyond the reach of current natural language understanding systems (Talmor et al., 2021).
Benchmarks. We consider ﬁve datasets covering a diverse range of commonsense reasoning types.
The popular CSQA (Talmor et al., 2019) asks commonsense questions about the world involving
complex semantics that often require prior knowledge. StrategyQA (Geva et al., 2021) requires
models to infer a multi-hop strategy to answer questions. We choose two specialized evaluation sets
from the BIG-bench effort (BIG-bench collaboration, 2021): Date Understanding, which involves
inferring a date from a given context, and Sports Understanding, which involves determining whether
a sentence relating to sports is plausible or implausible. Finally, the SayCan dataset (Ahn et al.,
2022) involves mapping a natural language instruction to a sequence of robot actions from a discrete
set. Figure 3 shows examples with chain of thought annotations for all datasets.
Prompts. We follow the same experimental setup as the prior section. For CSQA and StrategyQA,
we randomly selected examples from the training set and manually composed chains of thought for
them to use as few-shot exemplars. The two BIG-bench tasks do not have training sets, so we selected
the ﬁrst ten examples as exemplars in the evaluation set as few-shot exemplars and report numbers on
the rest of the evaluation set. For SayCan, we use six examples from the training set used in Ahn et al.
(2022) and also manually composed chains of thought.
Results. Figure 7 highlights these results for PaLM (full results for LaMDA, GPT-3, and different
model scales are shown in Table 4). For all tasks, scaling up model size improved the performance
of standard prompting; chain-of-thought prompting led to further gains, with improvements appear-
ing to be largest for PaLM 540B. With chain-of-thought prompting, PaLM 540B achieved strong
performance relative to baselines, outperforming the prior state of the art on StrategyQA (75.6% vs
69.4%) and outperforming an unaided sports enthusiast on sports understanding (95.4% vs 84%).
These results demonstrate that chain-of-thought prompting can also improve performance on tasks
requiring a range of commonsense reasoning abilities (though note that gain was minimal on CSQA).
86254020406080100 Solve rate (%)CSQA
8625405060708090StrategyQA
Standard prompting
Chain of thought
Prior supervised best
Human
862540020406080
Model scale (# parameters in billions)Date
862540406080100Sports
86254020406080100SayCan
Figure 7: Chain-of-thought prompting also improves the commonsense reasoning abilities of
language models. The language model shown here is PaLM. Prior best numbers are from the
leaderboards of CSQA (Talmor et al., 2019) and StrategyQA (Geva et al., 2021) (single-model only,
as of May 5, 2022). Additional results using various sizes of LaMDA, GPT-3, and PaLM are shown
in Table 4.
2We sample examples 60tokens to ﬁt into our input context window, and also limit the examples to 2
steps to solve for a fair comparison with the eight exemplars that we composed.
75 Symbolic Reasoning
0255075100 Solve rate (%)Letter Concat: 2
(in domain)Letter Concat: 4
(OOD)Standard prompting
Chain-of-thought prompting
8 62 540406080100 Solve rate (%)Coin Flip: 2
(in domain)
8 62 540
Model scale (# parameters in billions)Coin Flip: 4
(OOD)
Figure 8: Using chain-of-thought
prompting facilitates generalization to
longer sequences in two symbolic rea-
soning tasks.Our ﬁnal experimental evaluation considers symbolic rea-
soning, which is simple for humans but potentially chal-
lenging for language models. We show that chain-of-
thought prompting not only enables language models to
perform symbolic reasoning tasks that are challenging in
the standard prompting setting, but also facilitates length
generalization to inference-time inputs longer than those
seen in the few-shot exemplars.
Tasks. We use the following two toy tasks.
•Last letter concatenation. This task asks the model
to concatenate the last letters of words in a name (e.g.,
“Amy Brown”!“yn” ). It is a more challenging version
of ﬁrst letter concatenation, which language models can
already perform without chain of thought.3We generate
full names by randomly concatenating names from the
top one-thousand ﬁrst and last names from name census
data ( https://namecensus.com/ ).
•Coin ﬂip. This task asks the model to answer whether a
coin is still heads up after people either ﬂip or don’t ﬂip
the coin (e.g., “A coin is heads up. Phoebe ﬂips the coin.
Osvaldo does not ﬂip the coin. Is the coin still heads up?”
!“no” ).
As the construction of these symbolic reasoning tasks is
well-deﬁned, for each task we consider an in-domain test
set for which examples had the same number of steps as
the training/few-shot exemplars, as well as an out-of-domain (OOD) test set, for which evaluation
examples had more steps than those in the exemplars. For last letter concatenation, the model only
sees exemplars of names with two words, and then performs last letter concatenation on names with 3
and 4 words.4We do the same for the number of potential ﬂips in the coin ﬂip task. Our experimental
setup uses the same methods and models as in the prior two sections. We again manually compose
chains of thought for the few-shot exemplars for each task, which are given in Figure 3.
Results. The results of these in-domain and OOD evaluations are shown in Figure 8 for PaLM,
with results for LaMDA shown in Appendix Table 5. With PaLM 540B, chain-of-thought prompting
leads to almost 100% solve rates (note that standard prompting already solves coin ﬂip with PaLM
540, though not for LaMDA 137B). Note that these in-domain evaluations are “toy tasks” in the
sense that perfect solution structures are already provided by the chains of thought in the few-shot
exemplars; all the model has to do is repeat the same steps with the new symbols in the test-time
example. And yet, small models still fail—the ability to perform abstract manipulations on unseen
symbols for these three tasks only arises at the scale of 100B model parameters.
As for the OOD evaluations, standard prompting fails for both tasks. With chain-of-thought prompting,
language models achieve upward scaling curves (though performance is lower than in the in-domain
setting). Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of
thought for language models of sufﬁcient scale.
6 Discussion
We have explored chain-of-thought prompting as a simple mechanism for eliciting multi-step rea-
soning behavior in large language models. We ﬁrst saw that chain-of-thought prompting improves
performance by a large margin on arithmetic reasoning, yielding improvements that are much stronger
than ablations and robust to different annotators, exemplars, and language models (Section 3). Next,
3We tested 10 common names using GPT-3 davinci and it got all but one correct.
4For names of length longer than 2 words, we concatenate multiple ﬁrst and last names together.
8experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought
reasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,
chain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In
all experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language
model. No language models were ﬁnetuned in the process of writing this paper.
The emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme
(Wei et al., 2022b). For many reasoning tasks where standard prompting has a ﬂat scaling curve, chain-
of-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting
appears to expand the set of tasks that large language models can perform successfully—in other
words, our work underscores that standard prompting only provides a lower bound on the capabilities
of large language models. This observation likely raises more questions than it answers—for instance,
how much more can we expect reasoning ability to improve with a further increase in model scale?
What other prompting methods might expand the range of tasks that language models can solve?
As for limitations, we ﬁrst qualify that although chain of thought emulates the thought processes of
human reasoners, this does not answer whether the neural network is actually “reasoning,” which
we leave as an open question. Second, although the cost of manually augmenting exemplars with
chains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for
ﬁnetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot
generalization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct
and incorrect answers; improving factual generations of language models is an open direction for
future work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, inter alia ). Finally,
the emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in
real-world applications; further research could explore how to induce reasoning in smaller models.
7 Related Work
This work is inspired by many research areas, which we detail in an extended related work section
(Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.
The ﬁrst relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017)
pioneer the idea of using natural language rationales to solve math word problems through a series
of intermediate steps. Their work is a remarkable contrast to the literature using formal languages
to reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe
et al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to ﬁnetune a pretrained
language model rather than training a model from scratch. In the domain of program synthesis,
Nye et al. (2021) leverage language models to predict the ﬁnal outputs of Python programs via
ﬁrst line-to-line predicting the intermediate computational results, and show that their step-by-step
prediction method performs better than directly predicting the ﬁnal outputs.
Naturally, this paper also relates closely to the large body of recent work on prompting. Since the
popularization of few-shot prompting as given by Brown et al. (2020), several general approaches
have improved the prompting ability of models, such as automatically learning prompts (Lester et al.,
2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang
et al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,
instructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the
outputs of language models with a chain of thought.
8 Conclusions
We have explored chain-of-thought prompting as a simple and broadly applicable method for enhanc-
ing reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense
reasoning, we ﬁnd that chain-of-thought reasoning is an emergent property of model scale that allows
sufﬁciently large language models to perform reasoning tasks that otherwise have ﬂat scaling curves.
Broadening the range of reasoning tasks that language models can perform will hopefully inspire
further work on language-based approaches to reasoning.
9Acknowledgements
We thank Jacob Devlin, Claire Cui, Andrew Dai, and Ellie Pavlick for providing feedback on the
paper. We thank Jacob Austin, Yuhuai Wu, Henryk Michalewski, Aitor Lewkowycz, Charles Sutton,
and Aakanksha Chowdhery for helpful discussions. We thank Sid Maxwell for notifying us about a
mistake in the manual error analysis in the original manuscript.
References
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea
Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. 2022. Do as I can, not as I
say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691 .
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh
Hajishirzi. 2019. MathQA: Towards interpretable math word problem solving with operation-
based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers) , Minneapolis, Minnesota. Association for Computational Linguistics.
Daniel Andor, Luheng He, Kenton Lee, and Emily Pitler. 2019. Giving BERT a calculator: Finding
operations and arguments with reading comprehension. EMNLP .
Jacob Andreas, Dan Klein, and Sergey Levine. 2018. Learning with latent language. NAACL .
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,
Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language
models. arXiv preprint arXiv:2108.07732 .
BIG-bench collaboration. 2021. Beyond the imitation game: Measuring and extrapolating the
capabilities of language models. In preparation .
Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, and Greg Durrett. 2021. Flexible generation of natural
language deductions. EMNLP .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. 2020. Language models are few-shot learners. NeurIPS .
Jonathon Cai, Richard Shin, and Dawn Song. 2017. Making neural programming architectures
generalize via recursion. ICLR .
Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI:
Natural language inference with natural language explanations. NeurIPS .
Howard Chen, Jacqueline He, Karthik Narasimhan, and Danqi Chen. 2022. Can rationalization
improve robustness? NAACL .
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating
large language models trained on code. arXiv preprint arXiv:2107.03374 .
Xinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, and Quoc V . Le. 2019. Neural
symbolic reader: Scalable integration of distributed and symbolic representations for reading
comprehension. ICLR .
Ting-Rui Chiang and Yun-Nung Chen. 2019. Semantically-aligned equation generation for solving
and reasoning math word problems. In Proceedings of the 2019 Conference of the North Ameri-
can Chapter of the Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers) , pages 2656–2668, Minneapolis, Minnesota. Association for
Computational Linguistics.
10Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over
language. IJCAI .
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher
Hesse, and John Schulman. 2021. Training veriﬁers to solve math word problems. arXiv preprint
arXiv:2110.14168 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language understanding. NAACL .
Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. 2019. Neural
logic machines. ICLR .
Dheeru Dua, Sameer Singh, and Matt Gardner. 2020. Beneﬁts of intermediate annotations in reading
comprehension. ACL.
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did
aristotle use a laptop? A question answering benchmark with implicit reasoning strategies. TACL .
Yuling Gu, Bhavana Dalvi Mishra, and Peter Clark. 2022. DREAM: Uncovering mental models
behind language models. NAACL .
Braden Hancock, Paroma Varma, Stephanie Wang, Martin Bringmann, Percy Liang, and Christopher
Ré. 2018. Training classiﬁers with natural language explanations. ACL.
Peter Hase and Mohit Bansal. 2022. When can models learn from explanations? a formal framework
for understanding the roles of explanation data. ACL.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,
and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv
preprint arXiv:2103.03874 .
Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning
to solve arithmetic word problems with verb categorization. EMNLP .
Zhanming Jie, Jierui Li, and Wei Lu. 2022. Learning to reason deductively: Math word problem
solving as complex relation extraction. arXiv preprint arXiv:2203.10316 .
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language
models. arXiv preprint arXiv:2001.08361 .
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016.
MAWPS: A math word problem repository. NAACL .
Andrew K. Lampinen, Ishita Dasgupta, Stephanie C.Y . Chan, Kory Matthewson, Michael Henry
Tessler, Antonia Creswell, James L. McClelland, Jane X. Wang, and Felix Hill. 2022. Can language
models learn from explanations in context? arXiv preprint arXiv:2204.02329 .
Yihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan, Bing Tian Dai, Yan Wang, Dongxiang Zhang,
and Ee-Peng Lim. 2021. MWPToolkit: An open-source framework for deep learning-based math
word problem solvers. arXiv preprint arXiv:2109.00799 .
Teven Le Scao and Alexander Rush. 2021. How many data points is a prompt worth? NAACL .
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efﬁcient
prompt tuning. EMNLP .
Iddo Lev, Bill MacCartney, Christopher Manning, and Roger Levy. 2004. Solving logic puzzles:
From robust processing to precise semantics. Proceedings of the 2nd Workshop on Text Meaning
and Interpretation .
Xiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning: Optimizing continuous prompts for generation.
ACL.
11Zhengzhong Liang, Steven Bethard, and Mihai Surdeanu. 2021. Explainable multi-hop verbal
reasoning through internal monologue. NAACL .
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale
generation: Learning to solve and explain algebraic word problems. ACL.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021.
Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language
processing. arXiv preprint arXiv:2107.13586 .
Bodhisattwa Prasad Majumder, Oana-Maria Camburu, Thomas Lukasiewicz, and Julian McAuley.
2021. Rationale-inspired natural language explanations with commonsense. arXiv preprint
arXiv:2106.13876 .
Ana Marasovi ´c, Iz Beltagy, Doug Downey, and Matthew E Peters. 2022. Few-shot self-rationalization
with natural language prompts. NAACL Findings .
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and
factuality in abstractive summarization. In ACL.
Shen Yun Miao, Chao Chun Liang, and Keh Yih Su. 2020. A diverse corpus for evaluating and
developing English math word problem solvers. ACL.
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke
Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work?
arXiv preprint arXiv:2202.12837 .
Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan.
2020. WT5?! Training text-to-text models to explain their predictions. arXiv preprint
arXiv:2004.14546 .
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David
Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work:
Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114 .
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to
follow instructions with human feedback. arXiv preprint arXiv:2203.02155 .
Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve
simple math word problems? NAACL .
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL .
Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and
Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473 .
Piotr Pi˛ ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving
BERT’s mathematical abilities by predicting the order of reasoning. ACL.
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models:
Methods, analysis & insights from training Gopher. arXiv preprint arXiv:2112.11446 .
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a uniﬁed
text-to-text transformer. Journal of Machine Learning Research , 21:1–67.
Dheeraj Rajagopal, Vidhisha Balachandran, Eduard H. Hovy, and Yulia Tsvetkov. 2021. SelfExplain:
A self-explaining architecture for neural text classiﬁers. EMNLP .
Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain
yourself! Leveraging language models for commonsense reasoning. ACL.
12Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. 2019. NumNet: Machine reading
comprehension with numerical reasoning. EMNLP .
Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Michael Collins, Dipanjan Das, Slav Petrov,
Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2021. Measuring attribution in natural language
generation models. arXiv preprint arXiv:2112.12870 .
Gabriel Recchia. 2021. Teaching autoregressive language models complex tasks by demonstration.
arXiv preprint arXiv:2109.02102 .
Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. 2022.
A recipe for arbitrary text style transfer with large language models. ACL.
Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond
the few-shot paradigm. Extended Abstracts of the 2021 CHI Conference on Human Factors in
Computing Systems .
Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. EMNLP .
Subhro Roy, Tim Vieira, and Dan Roth. 2015. Reasoning about Quantities in Natural Language.
TACL .
Mohammed Saeed, Naser Ahmadi, Preslav Nakov, and Paolo Papotti. 2021. RuleBERT: Teaching
soft rules to pre-trained language models. EMNLP .
Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai,
Antoine Chafﬁn, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2022. Multitask prompted
training enables zero-shot task generalization. ICLR .
Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. 2021.
Generate & rank: A multi-task framework for math word problems. In Findings of the Association
for Computational Linguistics: EMNLP 2021 .
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A
question answering challenge targeting commonsense knowledge. NAACL .
Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, and Jonathan Berant. 2020. Leap-of-
thought: Teaching pre-trained models to systematically reason over implicit knowledge. NeurIPS .
Alon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bhagavatula, Yoav Goldberg, Yejin Choi, and
Jonathan Berant. 2021. CommonsenseQA 2.0: Exposing the limits of ai through gamiﬁcation.
NeurIPS Track on Datasets and Benchmarks .
Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven
Zheng, Neil Houlsby, and Donald Metzler. 2022. Unifying language learning paradigms. arXiv
preprint arXiv:2205.05131 .
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze
Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. LaMDA: Language models for
dialog applications. arXiv preprint arXiv:2201.08239 .
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022a.
Self-consistency improves chain of thought reasoning in language models. arXiv preprint
arXiv:2203.11171 .
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana
Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. 2022b.
Benchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv preprint
arXiv:2204.07705 .
Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M. Dai, and Quoc V . Le. 2022a. Finetuned language models are zero-shot learners. ICLR .
13Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022b. Emergent abilities of large language
models. Transactions on Machine Learning Research .
Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi. 2022. Reframing
human-AI collaboration for generating free-text explanations. NAACL .
Sarah Wiegreffe and Ana Marasovi ´c. 2021. Teach me to explain: A review of datasets for explainable
NLP. NeurIPS .
Sarah Wiegreffe, Ana Marasovi ´c, and Noah A. Smith. 2021. Measuring association between labels
and free-text rationales. EMNLP .
Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina, Michael Terry, and
Carrie J Cai. 2022a. PromptChainer: Chaining large language model prompts through visual
programming. CHI Extended Abstracts .
Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022b. AI chains: Transparent and controllable
human-AI interaction by chaining large language model prompts. CHI.
Yujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan, and Milad Hashemi. 2020.
Neural execution engines: Learning to execute subroutines. NeurIPS .
Huihan Yao, Ying Chen, Qinyuan Ye, Xisen Jin, and Xiang Ren. 2021. Reﬁning language models
with compositional explanations. NeurIPS .
Xi Ye and Greg Durrett. 2022. The unreliability of explanations in few-shot in-context learning.
arXiv preprint arXiv:2205.03401 .
Yordan Yordanov, Vid Kocijan, Thomas Lukasiewicz, and Oana-Maria Camburu. 2021. Few-shot
out-of-domain transfer learning of natural language explanations. arXiv preprint arXiv:2112.06204 .
Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using “annotator rationales” to improve
machine learning for text categorization. NAACL .
Wojciech Zaremba and Ilya Sutskever. 2014. Learning to execute. arXiv preprint arXiv:1410.4615 .
Eric Zelikman, Yuhuai Wu, and Noah D. Goodman. 2022. STaR: Bootstrapping reasoning with
reasoning. arXiv preprint arXiv:2203.14465 .
Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use:
Improving few-shot performance of language models. ICML .
Wangchunshu Zhou, Jinyi Hu, Hanlin Zhang, Xiaodan Liang, Maosong Sun, Chenyan Xiong, and
Jian Tang. 2020. Towards interpretable natural language understanding with explanations as latent
variables. NeurIPS .
14Checklist
1. For all authors...
(a)Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
contributions and scope? [Yes]
(b)Did you describe the limitations of your work? [Yes] See Section 6 and Appendix A.2.
(c)Did you discuss any potential negative societal impacts of your work? [Yes] We don’t
expect negative societal impacts as a direct result of the contributions in our paper. One
consideration, however, is that generated chain of thought is not always factual, which
is noted as a limitation in Appendix D.1 (and note that we do not suggest using such
chains of thought in a factual manner or in any real-world scenario).
(d)Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes]
2. If you are including theoretical results...
(a) Did you state the full set of assumptions of all theoretical results? [N/A]
(b) Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments...
(a)Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [Yes] We included
inputs, outputs, and targets for LaMDA and GPT-3 in the supplementary material.
Although we use proprietary models, we GPT-3 results are fully reproducible. Repro-
ducibility is further discussed in Appendix E.1.
(b)Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [Yes] Data splits were speciﬁed, N/A for hyperparams.
(c)Did you report error bars (e.g., with respect to the random seed after running exper-
iments multiple times)? [Yes] Standard deviation for multiple seeds using LaMDA
137B, where each seed is a different random order of exemplars, is given in Table 6
and Table 7.
(d)Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [Yes] Type of resources are described in
Appendix E.2, though we did not estimate the total amount of compute.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a)If your work uses existing assets, did you cite the creators? [Yes] We used two models
that we anonymized based on the recommendation of the NeurIPS chairs. These models
will be cited in the camera-ready version of the paper.
(b) Did you mention the license of the assets? [Yes] See Appendix E.3.
(c)Did you include any new assets either in the supplemental material or as a URL? [Yes]
The coinﬂip and last letter concatenation datasets are the only new assets, and they are
given in the Supplementary Materials.
(d)Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? [N/A] No human data collected.
(e)Did you discuss whether the data you are using/curating contains personally identiﬁable
information or offensive content? [N/A] No human data collected.
5. If you used crowdsourcing or conducted research with human subjects...
(a)Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A]
(b)Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A]
(c)Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A]
15A Frequently Asked Questions
A.1 Why does increasing model scale improve chain-of-thought prompting?
The ﬁnding that successful chain-of-thought reasoning predictably emerges only at certain model
scales is intriguing. Scaling up language models has been shown to confer beneﬁts such as improved
performance and sample efﬁciency (Kaplan et al., 2020), but chain-of-thought reasoning is emergent
in the sense that its success cannot be predicted only by extrapolating the performance of small scale
models, as chain of thought actually hurts performance for most models smaller than 10B parameters.
The question of why model scale improves chain-of-thought prompting is certainly multi-faceted, and
we made a preliminary attempt to shed insight into it via error analysis. This small analysis involved
manually reading 45 errors made by PaLM 62B and categorizing them into semantic understanding
(20 errors), one step missing (18 errors), and other errors (7 errors). The “other category” included
hallucinations, repetitive outputs, and symbol mapping errors. This categorization is a coarse one
borrowed from the initial error analysis done on LaMDA in Appendix D.2, for which categories were
conceived based on what improvements were needed to make the chain of thought correct.
As shown in Figure 9, scaling PaLM to 540B parameters ﬁxed a substantial portion of errors in all
three categories. Examples of semantic understanding and one-step missing errors that were ﬁxed by
scaling PaLM to 540B are given in Figure 10. This result appears consistent with a hypothesis that
language models acquire a range of semantic understanding and logical reasoning skills as a function
of model scale (though note that model scale is often conﬂated with other factors, such as amount of
training compute).
Semantic understanding
(62B made 20 errors of this type, 540B ﬁxes 6 of them)One step missing
(62B made 18 errors of this type, 540B ﬁxes 12 of them)Other
(62B made 7 errors of this type, 540B ﬁxes 4 of them)Types of errors made by a 62B language model:Errors ﬁxed by scaling from 62B to 540B
Figure 9: Error analysis of 45 problems that PaLM 62B got incorrect. These errors were categorized
that semantic understanding, one step missing, and other. The other category includes hallucinations,
repetitive outputs, and symbol mapping errors. Scaling PaLM to 540B ﬁxed a substantial portion of
errors in all categories.
There are also three notable points regarding why small language models fail. The ﬁrst observation
is that small language models fail at even relatively easy symbol mapping tasks. As demonstrated
in Section 5, for even symbolic reasoning tasks that only require generalization to new examples
using the same chain of thought logical structure that was given in the few-shot exemplars, small
language models still failed. The second observation is that small language models seem to have
inherently weaker arithmetic abilities, as shown by Brown et al. (2020), the ability to do simple
arithmetic operations (without semantic understanding) requires sufﬁcient model scale. Finally, we
noticed qualitatively that small language models often did not generate a ﬁnal answer that could be
parsed, due to either repetitions or logic that never arrived at a ﬁnal answer.
In summary, the success of chain-of-thought reasoning as a result of model scale is a complicated
phenomena that likely involves a variety of emergent abilities (semantic understanding, symbol
mapping, staying on topic, arithmetic ability, faithfulness, etc). Future work could more thoroughly
investigate what properties of pretraining data, model architecture, and optimization objective causally
enable such reasoning capabilities.
16Tracy used a piece of wire 4 feet long to support tomato plants in the garden. The wire was cut into pieces 6 inches long. How many pieces did she obtain?She cut the wire into 6 inch pieces. This means she got 4 * 6 = 24 pieces. The answer is 24. (Incorrect: semantic understanding error)62B Model OutputQuestion540B Model OutputThe wire was 4 feet long. This means it was 4 * 12 = 48 inches long. It was cut into pieces 6 inches long. This means she obtained 48 / 6 = 8 pieces. The answer is 8. (Correct)Tom's ship can travel at 10 miles per hour. He is sailing from 1 to 4 PM. He then travels back at a rate of 6 mph. How long does it take him to get back?He travels from 1 to 4 PM. That means he travels for 3 hours. He travels back at 6 mph. That means he travels for 3 * 6 = 18 miles. The answer is 18. (Incorrect: semantic understanding error)He travels at 10 mph from 1 to 4 PM. This means he travels 3 hours. 3 hours at 10 mph means he travels 3 * 10 = 30 miles. He then travels back at 6 mph. This means he travels 6 miles per hour. He has to travel 30 miles, so it takes him 30 / 6 = 5 hours. The answer is 5. (Correct)Stephen placed an online order for groceries. His final bill came to $40.00. Because this was through a delivery vendor, they tacked on a 25% fee to his final total and charged him $3.00 in delivery fees. Stephen also added a $4.00 tip. After the extra fees, what was the final price of Stephen's groceries?The final price of Stephen's groceries was $40.00. The delivery vendor added 25% to the final total, so the final total was 125% of $40.00. This means the final total was 125% * $40.00 = $50.00. The delivery vendor also charged $3.00 in delivery fees. So the final price of Stephen's groceries was $50.00 + $3.00 = $53.00. The answer is $53.00. (Incorrect: one step missing error)Stephen's bill was $40.00. The delivery vendor added 25% to the bill. This means the bill was increased by 25% of $40.00. This is 0.25 * $40.00 = $10.00. The delivery fee was $3.00. The tip was $4.00. So the total bill was $40.00 + $10.00 + $3.00 + 540b model $4.00 = $57.00. The answer is 57. (Correct)There are four schools competing at a basketball tournament. Each school has sent a girls’ basketball team and a boys’ basketball team and each team has 5 players each. Each school has also sent a coach for each team. In total, how many people have all of the schools sent?There are 4 schools. Each school has sent a girls’ basketball team and a boys’ basketball team. Each team has 5 players each. That means there are 4 * 2 * 5 = 40 players. Each team has a coach. That means there are 40 + 4 = 44 coaches. The answer is 44. (Incorrect: one step missing error)Each school has sent 2 teams, each with 5 players. This means each school has sent 2 * 5 = 10 players. Each school has also sent 2 coaches. This means each school has sent 10 + 2 = 12 people. There are 4 schools, so in total all of the schools have sent 4 * 12 = 48 people. The answer is 48. (Correct)62B Model Output
62B Model Output
62B Model Output540B Model Output
540B Model Output
540B Model OutputQuestion
Question
QuestionFigure 10: Examples of semantic understanding and one-step missing errors that were ﬁxed by
scaling PaLM from 62B to 540B.
A.2 What is the role of prompt engineering?
One of the key considerations of prompting is sensitivity to the exact prompt. There is no shortage
of work showing that prompts affect language models in unexpected ways (Min et al., 2022). The
general way that we created chain of thought annotations was by taking eight exemplars from the
training set and decomposing the reasoning process into multiple steps leading to the ﬁnal answer.
Examples of chain of thought annotations are provided in Figure 3, with full prompts given in
Appendix G. To analyze how sensitive chain of thought is to prompt engineering, we performed
robustness experiments with respect to various factors.
•Different annotators. We ﬁrst analyze robustness to three different annotators (Section 3.4 and
Figure 6). Although there is notable variance in performance (which we will discuss later), chain
of thought performed better than the baseline by a large margin for all three annotators on eight
datasets in arithmetic, commonsense, and symbolic reasoning (Table 6 and Table 7). Similar to the
annotation process in Cobbe et al. (2021), annotators were not given speciﬁc instructions about
17how to write the chain of thought annotations other than to simply write the step-by-step reasoning
process that led to the ﬁnal answer. Thus, the annotations were written in each annotator’s own
linguistic “chain of thought” writing style.
•Annotators without machine learning background. The GSM8K dataset (Cobbe et al., 2021)
conveniently provides a training set with reasoning chains written by crowd compute workers,
which enables us to investigate whether chain of thought still works with reasoning chains from an
independent source without a background in machine learning. So we randomly sampled three sets
of eight exemplars with chains of thought from GSM8K. These chain of thought annotations also
outperformed the baseline by a large margin for all four arithmetic datasets (Table 6), indicating
that chain of thought is not dependent on a particular set of annotators.
•Different exemplars. The different GSM8K exemplars experiment above (Table 6) also shows
that chain-of-thought prompting works for different sets of exemplars. Notably, we test every set of
exemplars on all four arithmetic datasets (instead of picking exemplars from the training set for
each dataset), which suggests that the exemplars do not necessarily have to come from the same
dataset distribution as the test examples.
•Different order of exemplars. Prior work has shown that in some cases (e.g., classiﬁcation) even
the order of prompts matter—varying the permutation of few-shot exemplars can cause the accuracy
of GPT-3 on SST-2 to range from near chance (54.3%) to near SOTA (93.4%) (Zhao et al., 2021).
We show the standard deviation of performance from different exemplars in Table 6 and Table 7.
Standard deviations with respect to prompt order are relatively minimal in almost all cases. The
one exception is the coin ﬂip task, for which exemplar orders have high standard deviation, likely
for the reason cited in Zhao et al. (2021)—for classiﬁcation, many exemplars of the same category
in a row biases the model outputs).
•Different number of exemplars. We also found that gains from chain-of-thought prompting
generally still held when there was a varying number of few-shot exemplars. This is shown for ﬁve
datasets in Figure 11 (we did not have the compute to run this for all datasets). We also found in
preliminary experiments that further increasing the number of exemplars in standard prompting
did not lead to signiﬁcant gains (e.g., increasing from 8 to 16 exemplars did not improve the
performance of standard prompting enough to catch up with chain-of-thought prompting).
•Different language models. Another interesting question is whether certain prompts that work
better for one model work better for other large language models. We ﬁnd that with the same
prompts, chain-of-thought prompting improves performance across all three models (LaMDA,
GPT-3, and PaLM) for all datasets except CSQA and StrategyQA for GPT-3 (Table 1, Table 4,
Table 5). The fact that gains from chain of thought did not transfer perfectly among models is
a limitation; further work could investigate why how different pre-training datasets and model
architectures affect the performance gain from chain-of-thought prompting.
Prompt engineering still matters, though. Although the results are relatively robust to the prompt
for arithmetic reasoning, we want to be clear that prompt engineering still does matter, and can
improve performance signiﬁcantly in many cases. Though most chain of thought annotations
outperform standard prompting, there is large variation in many cases. For instance, for the coin
ﬂip task, the performance varied from 99.6% for Annotator A to 71.4% for Annotator C, though
both were above standard prompting = 50.0% (see Table 7). There are even tasks where prompt
engineering is a requirement for good performance. In preliminary experiments, we tried using chain
of thought to enable language models to reverse the order of a list of 5 items. While two co-authors
were not able to write chain of thought prompts that solved the task despite their best attempts, a third
co-author was able to write a chain of thought that perfectly solved the task.
How to generate chain of thought annotations in a robust fashion could be an interesting direction
for future work. For instance, an idea here could be to use a large language model to automatically
generate chains of thought via prompting (and potentially optimize this over a validation set).
A.3 Will chain-of-thought prompting improve performance for my task of interest?
While chain-of-thought prompting is in principle applicable for any text-to-text task, it is more
helpful for some tasks than others. Based on the experiments in this paper, our intuition is that chain
of thought helps the most when three conditions are met: (1) the task is challenging and requires
18multi-step reasoning, (2) a large language model is used, and (3) the scaling curve is relatively ﬂat.
Conversely, the beneﬁts are smaller when one or more of these conditions are not met.
These intuitions are perhaps supported by the arithmetic reasoning results. The performance gain
from chain-of-thought prompting is largest for PaLM 540B on GSM8K (challenging multi-step
problems, ﬂat scaling curve), which meets these conditions. The performance gain is small for the
subsets of MAWPS that only require one or two steps (SingleOP, SingleEq, and AddSub), for which
PaLM 540B already achieves performance of 90% or higher (and it is also generally true that there is
less headroom for improvement when performance is already strong).
Although in this paper we focused on multi-step reasoning tasks (arithmetic, commonsense, and
symbolic), chain-of-thought prompting can potentially be applied to any task for which humans use a
“chain of thought” to solve (at least in principle). We leave the empirical evaluation of chain-of-thought
prompting on such diverse tasks (e.g., machine translation, etc.) to future work.
A.4 Why is prompting with the equation only not enough for some arithmetic reasoning
datasets?
Prompting with the equation only as an intermediate step does help on many datasets, especially when
the datasets only require a few reasoning steps (SV AMP, ASDiv, MAWPS). For GSM8K, however,
using the equation only did not improve performance substantially. Based on qualitative analysis, we
believe that these questions are too semantically challenging for the model to directly translate them
into a math equation. Consider this example from LaMDA 137B:
QUESTION :Mike plays ping pong for 40 minutes. In the ﬁrst 20 minutes, he scores 4
points. In the second 20 minutes, he scores 25% more points. How many total points did he
score?
EQUATION ONLY (WRONG ANSWER ):(4 + 20 * 0.25) = 6. The answer is 6.
CHAIN OF THOUGHT (CORRECT ):Mike played ping pong for 40 minutes. In the ﬁrst
20 minutes, he scored 4 points. In the second 20 minutes, he scored 25% more points. So
he scored 25% more in the second 20 minutes. 4 x 1.25 = 5. So he scored 5 points in the
second 20 minutes. So he scored 9 points in total. The answer is 9.
It is hard for the model to directly translate all of the semantics into a single equation, but chain of
thought allows it to better reason about each part of the question via intermediate steps in natural
language.
19B All Experimental Results
This section contains tables for experimental results for varying models and model sizes, on all
benchmarks, for standard prompting vs. chain-of-thought prompting.
For the arithmetic reasoning benchmarks, some chains of thought (along with the equations produced)
were correct, except the model performed an arithmetic operation incorrectly. A similar observation
was made in Cobbe et al. (2021). Hence, we can further add a Python program as an external
calculator (using the Python eval function) to all the equations in the generated chain of thought.
When there are multiple equations in a chain of thought, we propagate the external calculator results
from one equation to the following equations via string matching. As shown in Table 1, we see that
adding a calculator signiﬁcantly boosts performance of chain-of-thought prompting on most tasks.
Table 1: Chain of thought prompting outperforms standard prompting for various large language
models on ﬁve arithmetic reasoning benchmarks. All metrics are accuracy (%). Ext. calc.: post-hoc
external calculator for arithmetic computations only. Prior best numbers are from the following. a:
Cobbe et al. (2021). b&e: Pi et al. (2022), c: Lan et al. (2021), d: Pi˛ ekos et al. (2021).
Prompting GSM8K SV AMP ASDiv AQuA MAWPS
Prior best N/A (ﬁnetuning) 55a57.4b75.3c37.9d88.4e
UL2 20B Standard 4.1 10.1 16.0 20.5 16.6
Chain of thought 4.4 (+0.3) 12.5 (+2.4) 16.9 (+0.9) 23.6 (+3.1) 19.1 (+2.5)
+ ext. calc 6.9 28.3 34.3 23.6 42.7
LaMDA 137B Standard 6.5 29.5 40.1 25.5 43.2
Chain of thought 14.3 (+7.8) 37.5 (+8.0) 46.6 (+6.5) 20.6 (-4.9) 57.9 (+14.7)
+ ext. calc 17.8 42.1 53.4 20.6 69.3
GPT-3 175B Standard 15.6 65.7 70.3 24.8 72.7
(text-davinci-002) Chain of thought 46.9 (+31.3) 68.9 (+3.2) 71.3 (+1.0) 35.8 (+11.0) 87.1 (+14.4)
+ ext. calc 49.6 70.3 71.1 35.8 87.5
Codex Standard 19.7 69.9 74.0 29.5 78.7
(code-davinci-002) Chain of thought 63.1 (+43.4) 76.4 (+6.5) 80.4 (+6.4) 45.3 (+15.8) 92.6 (+13.9)
+ ext. calc 65.4 77.0 80.0 45.3 93.3
PaLM 540B Standard 17.9 69.4 72.1 25.2 79.2
Chain of thought 56.9 (+39.0) 79.0 (+9.6) 73.9 (+1.8) 35.8 (+10.6) 93.3 (+14.2)
+ ext. calc 58.6 79.8 72.6 35.8 93.5
20Table 2: Standard prompting versus chain of thought prompting on ﬁve arithmetic reasoning bench-
marks. Note that chain of thought prompting is an emergent ability of model scale—it does not
positively impact performance until used with a model of sufﬁcient scale.
GSM8K SV AMP ASDiv AQuA MAWPS
Model standard CoT standard CoT standard CoT standard CoT standard CoT
UL2 20B 4.1 4.4 10.1 12.5 16.0 16.9 20.5 23.6 16.6 19.1
LaMDA 420M 2.6 0.4 2.5 1.6 3.2 0.8 23.5 8.3 3.2 0.9
2B 3.6 1.9 3.3 2.4 4.1 3.8 22.9 17.7 3.9 3.1
8B 3.2 1.6 4.3 3.4 5.9 5.0 22.8 18.6 5.3 4.8
68B 5.7 8.2 13.6 18.8 21.8 23.1 22.3 20.2 21.6 30.6
137B 6.5 14.3 29.5 37.5 40.1 46.6 25.5 20.6 43.2 57.9
GPT 350M 2.2 0.5 1.4 0.8 2.1 0.8 18.1 8.7 2.4 1.1
1.3B 2.4 0.5 1.5 1.7 2.6 1.4 12.6 4.3 3.1 1.7
6.7B 4.0 2.4 6.1 3.1 8.6 3.6 15.4 13.4 8.8 3.5
175B 15.6 46.9 65.7 68.9 70.3 71.3 24.8 35.8 72.7 87.1
Codex - 19.7 63.1 69.9 76.4 74.0 80.4 29.5 45.3 78.7 92.6
PaLM 8B 4.9 4.1 15.1 16.8 23.7 25.2 19.3 21.7 26.2 30.5
62B 9.6 29.9 48.2 46.7 58.7 61.9 25.6 22.4 61.8 80.3
540B 17.9 56.9 69.4 79.0 72.1 73.9 25.2 35.8 79.2 93.3
Table 3: Standard prompting versus chain of thought prompting on the four subsets of the MAWPS
benchmark. The point of stratifying the MAWPS benchmark is to show that performance gains are
minimal on easy one-step or two-step problems where large language models already achieve high
performance (e.g., SingleOp, SingleEq, and AddSub).
SingleOp SingleEq AddSub MultiArith
Model standard CoT standard CoT standard CoT standard CoT
UL2 20B 24.9 27.2 18.0 20.2 18.5 18.2 5.0 10.7
LaMDA 420M 2.8 1.0 2.4 0.4 1.9 0.7 5.8 1.5
2B 4.6 4.1 2.4 3.3 2.7 3.2 5.8 1.8
8B 8.0 7.0 4.5 4.4 3.4 5.2 5.2 2.4
68B 36.5 40.8 23.9 26.0 17.3 23.2 8.732.4
137B 73.2 76.2 48.8 58.7 43.0 51.9 7.644.9
GPT 350M 3.2 1.8 2.0 0.2 2.0 1.5 2.3 0.8
1.3B 5.3 3.0 2.4 1.6 2.3 1.5 2.2 0.5
6.7B 13.5 3.9 8.7 4.9 8.6 2.5 4.5 2.8
175B 90.9 88.8 82.7 86.6 83.3 81.3 33.8 91.7
Codex - 93.1 91.8 86.8 93.1 90.9 89.1 44.0 96.2
PaLM 8B 41.8 46.6 29.5 28.2 29.4 31.4 4.215.8
62B 87.9 85.6 77.2 83.5 74.7 78.2 7.373.7
540B 94.1 94.1 86.5 92.3 93.9 91.9 42.2 94.7
21Table 4: Standard prompting versus chain of thought prompting on ﬁve commonsense reasoning
benchmarks. Chain of thought prompting is an emergent ability of model scale—it does not positively
impact performance until used with a model of sufﬁcient scale.
CSQA StrategyQA Date Sports SayCan
Model standard CoT standard CoT standard CoT standard CoT standard CoT
UL2 20B 34.2 51.4 59.0 53.3 13.5 14.0 57.9 65.3 20.0 41.7
LaMDA 420M 20.1 19.2 46.4 24.9 1.9 1.6 50.0 49.7 7.5 7.5
2B 20.2 19.6 52.6 45.2 8.0 6.8 49.3 57.5 8.3 8.3
8B 19.0 20.3 54.1 46.8 9.5 5.4 50.0 52.1 28.3 33.3
68B 37.0 44.1 59.6 62.2 15.5 18.6 55.2 77.5 35.0 42.5
137B 53.6 57.9 62.4 65.4 21.5 26.8 59.5 85.8 43.3 46.6
GPT 350M 14.7 15.2 20.6 0.9 4.3 0.9 33.8 41.6 12.5 0.8
1.3B 12.0 19.2 45.8 35.7 4.0 1.4 0.0 26.9 20.8 9.2
6.7B 19.0 24.0 53.6 50.0 8.9 4.9 0.0 4.4 17.5 35.0
175B 79.5 73.5 65.9 65.4 43.8 52.1 69.6 82.4 81.7 87.5
Codex - 82.3 77.9 67.1 73.2 49.0 64.8 71.7 98.5 85.8 88.3
PaLM 8B 19.8 24.9 55.6 53.5 12.9 13.1 55.1 75.2 34.2 40.0
62B 65.4 68.1 58.4 63.4 29.8 44.7 72.1 93.6 65.8 70.0
540B 78.1 79.9 68.6 77.8 49.0 65.3 80.5 95.4 80.8 91.7
Table 5: Standard prompting versus chain of thought prompting enables length generalization to
longer inference examples on two symbolic manipulation tasks.
Last Letter Concatenation Coin Flip (state tracking)
2 OOD: 3 OOD: 4 2 OOD: 3 OOD: 4
Model standard CoT standard CoT standard CoT standard CoT standard CoT standard CoT
UL2 20B 0.6 18.8 0.0 0.2 0.0 0.0 70.4 67.1 51.6 52.2 48.7 50.4
LaMDA 420M 0.3 1.6 0.0 0.0 0.0 0.0 52.9 49.6 50.0 50.5 49.5 49.1
2B 2.3 6.0 0.0 0.0 0.0 0.0 54.9 55.3 47.4 48.7 49.8 50.2
8B 1.5 11.5 0.0 0.0 0.0 0.0 52.9 55.5 48.2 49.6 51.2 50.6
68B 4.4 52.0 0.0 0.8 0.0 2.5 56.2 83.2 50.4 69.1 50.9 59.6
137B 5.8 77.5 0.034.4 0.013.5 49.0 99.6 50.7 91.0 49.1 74.5
PaLM 8B 2.6 18.8 0.0 0.0 0.0 0.2 60.0 74.4 47.3 57.1 50.9 51.8
62B 6.8 85.0 0.059.6 0.013.4 91.4 96.8 43.9 91.0 38.3 72.4
540B 7.6 99.4 0.294.8 0.063.0 98.1 100.0 49.3 98.6 54.8 90.2
22Table 6: Ablation and robustness results for arithmetic reasoning datasets. Chain of thought generally
outperforms ablations by a large amount. “Equation only” performs in between standard prompting
and chain of thought prompting, as it allows for intermediate reasoning steps via equations but does
not leverage natural language. Chain of thought prompting has variance (as expected) when used
with prompts written by different annotators or when using other exemplars, but still outperforms
standard prompting by a large margin. Standard deviation shown is for different order of few-shot
prompting exemplars, with ﬁve different random seeds. Results here are shown for LaMDA 137B, as
additional queries for GPT-3 and PaLM are both limited and expensive.
GSM8K SV AMP ASDiv MAWPS
Standard prompting 6.5 0.4 29.50.6 40.10.6 43.20.9
Chain of thought prompting 14.3 0.4 36.70.4 46.60.7 57.91.5
Ablations
equation only 5.4 0.2 35.10.4 45.90.6 50.11.0
variable compute only 6.4 0.3 28.00.6 39.40.4 41.31.1
reasoning after answer 6.1 0.4 30.70.9 38.60.6 43.61.0
Robustness
different annotator (B) 15.5 0.6 35.20.4 46.50.4 58.21.0
different annotator (C) 17.6 1.0 37.52.0 48.70.7 60.12.0
intentionally concise style 11.1 0.3 38.70.8 48.00.3 59.60.7
exemplars from GSM8K ( ) 12.6 0.6 32.81.1 44.10.9 53.91.1
exemplars from GSM8K ( ) 12.7 0.5 34.81.1 46.90.6 60.90.8
exemplars from GSM8K ( ) 12.6 0.7 35.60.5 44.42.6 54.24.7
Table 7: Ablation and robustness results for four datasets in commonsense and symbolic reasoning.
Chain of thought generally outperforms ablations by a large amount. Chain of thought prompting has
variance (as expected) when used with prompts written by different annotators or when using other
exemplars, but still outperforms standard prompting by a large margin. Standard deviation shown
is for different order of few-shot prompting exemplars, with ﬁve different random seeds. Results
here are shown for LaMDA 137B, as additional queries for GPT-3 and PaLM are both limited and
expensive. The exception is that we run SayCan using PaLM here, as the SayCan evaluation set is
only 120 examples and therefore less expensive to run multiple times.
Commonsense Symbolic
Date Sports SayCan Concat Coin
Standard prompting 21.5 0.6 59.53.0 80.81.8 5.80.6 49.02.1
Chain of thought prompting 26.8 2.1 85.81.8 91.71.4 77.53.8 99.60.3
Ablations
variable compute only 21.3 0.7 61.62.2 74.22.3 7.21.6 50.70.7
reasoning after answer 20.9 1.0 63.02.0 83.30.6 0.00.0 50.20.5
Robustness
different annotator (B) 27.4 1.7 75.42.7 88.31.4 76.01.9 77.57.9
different annotator (C) 25.5 2.5 81.13.6 85.01.8 68.12.2 71.411.1
23C Extended Related Work
Chain-of-thought prompting is a general approach that is inspired by several prior directions: prompt-
ing, natural language explanations, program synthesis/execution, numeric and logical reasoning, and
intermediate language steps.
C.1 Prompting
The recent success of large-scale language models has led to growing interest in improving their
capability to perform tasks via prompting (Brown et al. (2020), and see Liu et al. (2021) for a
survey). This paper falls in the category of general prompting approaches, whereby input prompts are
optimized to allow a single large language model to better perform a variety of tasks (Li and Liang,
2021; Lester et al., 2021; Reif et al., 2022, inter alia ).
One recent line of work aims to improve the ability of language models to perform a task by providing
instructions that describe the task (Raffel et al., 2020; Wei et al., 2022a; Ouyang et al., 2022; Sanh
et al., 2022; Wang et al., 2022b). This line of work is related because it also augments input–output
pairs with meta-data. But whereas an instruction augments the input to a task (instructions are typically
prepended to the inputs), chain-of-thought prompting augments the outputs of language models.
Another related direction is sequentially combining the outputs of language models; human–computer
interaction (HCI) work (Wu et al., 2022a,b) has shown that combining sequential generations of
language models improves task outcomes in a 20-person user study.
C.2 Natural language explanations
Another closely related direction uses natural language explanations (NLEs), often with the goal of
improving model interpretability (Zhou et al., 2020; Wiegreffe and Marasovi ´c, 2021, inter alia ). That
line of work typically focuses on natural language inference (Camburu et al., 2018; Yordanov et al.,
2021; Bostrom et al., 2021), and produces explanations either simultaneously to or after the ﬁnal
prediction (Narang et al., 2020; Majumder et al., 2021; Wiegreffe et al., 2021, 2022). By contrast,
the chain of thought processing considered in this paper occurs before the ﬁnal answer. And while
NLE aims mostly to improve neural network interpretability (Rajagopal et al., 2021), the goal of
chain-of-thought prompting is to allow models to decompose multi-hop reasoning tasks into multiple
steps—interpretability is just a side effect. Marasovi ´c et al. (2022) show that prompt-based ﬁnetuning
with NLE improves NLI and classiﬁcation performance, though they largely focus on evaluating
explanation plausibility. In comparison, our work focuses on a range of arithmetic, commonsense,
and symbolic tasks that require multi-hop reasoning.
C.3 Program synthesis and execution
Using intermediate reasoning steps has a long history in program synthesis and execution (Zaremba
and Sutskever, 2014, inter alia ). Recent work along in this direction has included a number of
architectural innovations (Cai et al., 2017; Dong et al., 2019; Yan et al., 2020), as well as the use of
large language models (Chen et al., 2021; Austin et al., 2021). The program execution work closest to
ours is perhaps Nye et al. (2021), which show that large language models can perform up to 10-digit
addition, evaluate polynomials, and execute python programs. Whereas generating a program and
then executing it can be viewed as a type of reasoning, our work generalizes such domain-speciﬁc
primitives to natural language, which is open-domain and relevant to any text-to-text NLP task in
principle.
C.4 Numeric and logical reasoning
Numeric and logical reasoning has been a long-studied task in machine learning and natural language
processing (Lev et al., 2004, inter alia ). Recent work has also aimed to inject numeric reasoning
abilities in language models in various ways, such as augmenting BERT with a predeﬁned set of
executable operations (Andor et al., 2019), including a graph neural network (Ran et al., 2019), and
using specialized training procedures (Pi˛ ekos et al., 2021). Another line of work aims to enable
language models to perform logical or formal reasoning, often by verablizing the rules in natural
language formal rules using language (Clark et al., 2020; Saeed et al., 2021; Liang et al., 2021).
24Perhaps the most-related work here is Recchia (2021), which shows that ﬁnetuning enables longhand
module operations, which has previously been difﬁcult for performers. Whereas work in this direction
is often task-speciﬁc and uses ﬁnetuning, we show that chain-of-thought prompting works for a broad
range of tasks without any ﬁnetuning.
C.5 Intermediate language steps
Extensive prior work has shown the beneﬁts of endowing neural networks with the ability to produce
intermediate steps via training or ﬁnetuning confers various beneﬁts in a range of scenarios. As
examples, it has been shown that natural language intermediate steps can improve performance
(Zaidan et al., 2007; Yao et al., 2021; Hase and Bansal, 2022; Gu et al., 2022), improve robustness
(Chen et al., 2022), speed up training (Hancock et al., 2018), mitigate bias (Dua et al., 2020), and
even help in image and reinforcement learning settings (Andreas et al., 2018). To endow models with
the ability to produce intermediate steps, prior work typically ﬁnetunes models on either manually
annotated training datasets (Camburu et al., 2018; Rajani et al., 2019, inter alia ) or generates synthetic
datasets (Talmor et al., 2020; Zelikman et al., 2022). Compared with these training or ﬁnetuning
methods, our work shows that various natural language reasoning abilities can be elicited in off-the-
shelf language models of sufﬁcient scale simply via prompting. This prompting setup is important
because it allows for intermediate step reasoning without a large number of labeled annotations, and
because a single model can perform a range of reasoning tasks without any gradient updates.
D Appendix: Additional Analysis
D.1 Correct Chain of Thought Analysis
As mentioned in the main text, we analyze 50 chains of thought from LaMDA 137B that led to
correct answers in the GSM8K dataset. Of these 50, only one arrived at the correct answer through
incorrect reasoning (shown in Table 9: “correct by chance”). The other 49 had correct logic and
math, with examples shown in Table 8. Five had minor imperfections while maintaining coherent and
understandable logic:
• One had underspeciﬁed statements (shown in Table 9: “correct but underspeciﬁed statements”).
•One made an unrelated but correct statement (shown in Table 9: “correct but unrelated statement”).
•Two had the correct logic but omitted speciﬁc steps in the equation, though it was still understand-
able (shown in Table 9: “correct but imperfect equation”).
•One had the correct math but inverted the semantics (shown in Table 9: “correct but inverted
semantics”)
Table 8: Example of correct chains of thought produced by the model for the GSM8K dataset.
QUESTION :James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint. How many total
meters does he run a week?
MODEL ANSWER (CORRECT): James decides to run 3 sprints 3 times a week. He runs 60 meters each
sprint. So he runs 60 meters x 3 sprints x 3 times a week. That is 60 meters x 9. The answer is 540. X
QUESTION :Brandon’s iPhone is four times as old as Ben’s iPhone. Ben’s iPhone is two times older than Suzy’s
iPhone. If Suzy’s iPhone is 1 year old, how old is Brandon’s iPhone?
MODEL ANSWER (CORRECT): Brandon’s iPhone is 4 times as old as Ben’s iPhone. Ben’s iPhone is 2
times older than Suzy’s iPhone. So Brandon’s iPhone is 4 x 2 = 8 times older than Suzy’s iPhone. Suzy’s iPhone
is 1 year old. So Brandon’s iPhone is 8 x 1 = 8 years old. The answer is 8. X
QUESTION :Jean has 30 lollipops. Jean eats 2 of the lollipops. With the remaining lollipops, Jean wants to
package 2 lollipops in one bag. How many bags can Jean ﬁll?
MODEL ANSWER (CORRECT): Jean started with 30 lollipops. She ate 2 of them. So she has 28 lollipops
left. She wants to package 2 lollipops in one bag. So she can package 28 / 2 = 14 bags. The answer is 14. X
Although we ﬁnd that chain-of-thought reasoning is mostly correct for math questions where the ﬁnal
answer was correct, this is likely because it is hard to arrive at the correct ﬁnal answer by coincidence
for free response questions. For multiple choice or binary classiﬁcation problems, it was much more
25likely that models could arrive at the correct answer via an incorrect reasoning path (e.g., all the
commmonsense reasoning datasets we evaluate). This is a limitation, and future work should perform
an analysis of the factuality of such chains of thought.
Table 9: Of 50 examples that the model got correct, there were 7 salient cases where the chain of
thought the model generated was imperfect. We found the other 43 to be equivalent to what a human
would potentially generate as a ground truth.
QUESTION :There are 36 penguins sunbathing in the snow. One-third of them jump in and swim in the ocean.
Another one-third go inside the cave to eat their dinner. How many penguins are still left sunbathing?
MODEL ANSWER (CORRECT BY CHANCE ):There are 36 penguins. One-third of them jump in and swim in
the ocean. So that is 12 penguins. Another one-third go inside the cave to eat their dinner. So that is 12 penguins.
The answer is 12.
QUESTION :John has 2 houses with 3 bedrooms each. Each bedroom has 2 windows each. There are an
additional 4 windows in each house not connected to bedrooms. How many total windows are there between the
houses?
MODEL ANSWER (CORRECT BUT UNDERSPECIFIED STATEMENTS ):There are 2 houses with 3 bedrooms
each. Each bedroom has 2 windows each. So there are 2 x 3 = 6 windows in each house. There are an additional
4 windows in each house not connected to bedrooms. So there are 4 + 4 = 8 windows in each house. So there are
6 x 2 = 12 windows in each house. So there are 12 + 8 = 20 windows in both houses. The answer is 20.
QUESTION :If Ann is 9 years old and her brother is twice her age, how old will her brother be in 3 years?
MODEL ANSWER (CORRECT BUT UNRELATED STATEMENT ):Ann is 9 years old. Her brother is twice her
age. So her brother is 18 years old. In 3 years, she will be 12. So her brother will be 18 + 3 = 21 years old. The
answer is 21.
QUESTION :A local gas station is selling gas for $3.00 a gallon. An app company is offering $.20 cashback per
gallon if you ﬁll up at this station. If someone buys 10 gallons of gas, how much with their gas be, after the
cashback rewards?
MODEL ANSWER (CORRECT BUT IMPERFECT EQUATION ):The gas is originally 3 dollars per gallon. If
you buy 10 gallons, that is 3 x 10 = 30 dollars. If you get 20 cents off per gallon, that is 20 x 10 = 2 dollars. So
the total is 30 - 2 = 28 dollars. The answer is 28 dollars.
QUESTION :Nissa hires 60 seasonal workers to play elves in her department store’s Santa village. A third of
the elves quit after children vomit on them, then 10 of the remaining elves quit after kids kick their shins. How
many elves are left?
MODEL ANSWER (CORRECT BUT IMPERFECT EQUATION ):Nissa hires 60 seasonal workers. A third of
them quit. So 60 - 1/3 = 40 elves are left. Then 10 elves quit. So 40 - 10 = 30 elves are left. The answer is 30
elves.
QUESTION :Grandma Jones baked 5 apple pies for the ﬁreman’s luncheon. She cut each pie into 8 pieces and
set the ﬁve pies out on the buffet table for the guests to serve themselves. At the end of the evening, after the
guests had taken and eaten their pieces of pie, there were 14 pieces of pie remaining. How many pieces were
taken by the guests?
MODEL ANSWER (CORRECT BUT INVERTED SEMANTICS ):5 pies were baked and cut into 8 pieces each.
The 5 pies were then served to the guests. 8 x 5 = 40 pieces of pie. The guests ate 14 pieces of pie. 40 - 14 = 26
pieces of pie were left. The answer is 26. X
26D.2 Incorrect Chain of Thought Analysis
We also manually analyze 50 randomly sampled outputs of the model that were incorrect on GSM8K
for LaMDA 137B. There are many ways that a chain of thought can be incorrect, making the design
of error categorization non-trivial. We decided to categorize errors into what changes are needed to
make the chain of thought correct, with the goal of elucidating how the model can be improved in the
future.
We found that many chains of thought can be made correct with one of the following three classes of
modiﬁcation.
•Calculator error only. We found that 8% of the chains of thought were completely correct except
for a calculator error—in other words, applying an external calculator to equations, as done in
Cobbe et al. (2021), would make the chain of thought correct. An example of this type of error is
shown in Table 10: “calculator error only”. Indeed, the solve rate of chain-of-thought prompting
on for LaMDA 137B GSM8K went up from 14.3% to 17.3% when we added a Python program
as an external calculator, as shown in Table 2. Also, 34% of the examples contained calculator
errors in addition to other types of errors. However, we perform the rest of the error categorization
independently of calculator errors.
•Symbol mapping error. We next found that 16% percent of the chains of thought were correct
except for what we call symbol mapping errors. We deﬁne a symbol mapping error as when the
chain of thought is correct except for the number symbols, and it could be made totally correct by
modifying only the equations and not the words. As one might argue that they could simply place
the correct ﬁnal equation in any chain of thought, we constrain this category to chains of thought
where the chain of thought can be modiﬁed to be a completely correct reasoning process (not just
ﬁnal answer). An example of this error category is shown in Table 10: “symbol mapping error”.
•One step missing error. Our next category of error is chains of thought which were correct except
that they were missing a single step. In other words, these chains of thoughts could be rewritten to
be correct by adding in an additional reasoning step that was missed by the model. An example of
this error category is shown in Table 10: “one step missing error”. We found that 22% percent of
the errors fell into this category.
Table 10: Example of incorrect chains of thought, categorized as described in Appendix D.2.
QUESTION :Dan plants 3 rose bushes. Each rose bush has 25 roses. Each rose has 8 thorns. How many thorns
are there total?
MODEL ANSWER (INCORRECT ;CALCULATOR ERROR ONLY ):Dan plants 3 rose bushes. Each rose bush
has 25 roses. Each rose has 8 thorns. So 3 x 25 x 8 = 300. The answer is 300. 7
EXPLANATION FOR ERROR CATEGORY :The produced chain of thought could be made correct just by
running the equation through an external calculator, instead of asking the model to do the computation.
QUESTION :Jill gets paid $20 per hour to teach and $30 to be a cheerleading coach. If she works 50 weeks a
year, 35 hours a week as a teacher and 15 hours a week as a coach, what’s her annual salary?
MODEL ANSWER (INCORRECT ;SYMBOL MAPPING ERROR ):Jill gets paid 20 dollars per hour to teach
and 30 dollars per hour to be a cheerleading coach. If she works 50 weeks a year, 35 hours a week as a teacher
and 15 hours a week as a coach, then she works 50 x 35 = 1750 hours as a teacher and 15 x 30 = 450 hours as a
coach. So she works 1750 + 450 = 2200 hours. She gets paid 20 dollars per hour for 1750 hours and 30 dollars
per hour for 450 hours. So her annual salary is 20 x 1750 + 30 x 450 = $36,500. The answer is $36,500. 7
EXPLANATION FOR ERROR CATEGORY :The model incorrectly mapped “15 x 30” instead of “15 x 50”, and
the chain of thought would be made correct by ﬁxing just that one change (with an external calculator).
QUESTION :Kelian has two recipes for preparing dishes, one having 20 instructions and the second one having
twice as many instructions as the ﬁrst one. How many instructions does Kelian have to read to prepare the two
dishes?
MODEL ANSWER (INCORRECT ;ONE STEP MISSING ERROR ):Kelian has two recipes. One has 20 instruc-
tions. The other has twice as many instructions as the ﬁrst one. So the second one has 40 instructions. So Kelian
has to read 40 instructions to prepare the two dishes. The answer is 40. 7
EXPLANATION FOR ERROR CATEGORY :The model answer could be made correct just adding one additional
step (in this case, adding 20 and 40).
27We found that the remaining chains of thought (27 of 50; 54%) would require substantial edits to make
into a correct chain of thought. Almost all cases here involved some error in semantic understanding
(see Table 11: “semantic understanding error”), and 8 of the 27 also had incoherent chain of thoughts,
meaning that some statements in the generated chain of thought did not follow from prior ones or
violated basic world knowledge (see Table 11: “incoherent chain of thought error”).
Table 11: Example of incorrect chains of thought, categorized as described in Appendix D.2. These
chains of thought had multiple mistakes that could not be ﬁxed with minor edits.
QUESTION :Gretchen has 110 coins. There are 30 more gold coins than silver coins. How many gold coins
does Gretchen have?
MODEL ANSWER (INCORRECT ;SEMANTIC UNDERSTANDING ERROR ):Gretchen has 110 coins. There
are 30 more gold coins than silver coins. So there are 110 - 30 = 80 silver coins. So there are 80 silver coins and
110 - 80 = 30 gold coins. The answer is 30. 7
EXPLANATION FOR ERROR CATEGORY :The model does not understand that 110 coins are in total and that
it must solve an equation to get the number of gold and silver coins.
QUESTION :In a dance class of 20 students, 20% enrolled in contemporary dance, 25% of the remaining
enrolled in jazz dance, and the rest enrolled in hip-hop dance. What percentage of the entire students enrolled in
hip-hop dance?
MODEL ANSWER (INCORRECT ;INCOHERENT CHAIN OF THOUGHT ERROR ):20% of the students en-
rolled in contemporary dance. 25% of the remaining students enrolled in jazz dance. The rest enrolled in hip-hop
dance. So the percentage of the entire students enrolled in hip-hop dance is the percentage of the students
enrolled in hip-hop dance, minus the percentage of the students enrolled in contemporary dance, minus the
percentage of the students enrolled in jazz dance. So the percentage of the entire students enrolled in hip-hop
dance is (25 + 20) - (25 + 20) = 100%. The answer is 100%. 7
EXPLANATION FOR ERROR CATEGORY :This chain of thought is incoherent in that the percent of entire
students enrolled in hip-hope dance cannot be the percent of student enrolled in hip-hop dance minus another
term.
Overall, there are no guarantees that the reasoning processes generated by large language models
are coherent or factually correct, as underscored by the recent work evaluating the factuality of
language model generations and explanations (Maynez et al., 2020; Rashkin et al., 2021; Ye and
Durrett, 2022; Marasovi ´c et al., 2022; Wiegreffe et al., 2022). Incorrect reasoning processes can lead
to both incorrect ﬁnal answers as well as accidentally correct ﬁnal answers (with accidentally correct
ﬁnal answers being more likely for tasks such as binary classiﬁcation as opposed to free response).
Improving the factuality of language model generations with respect to context and world knowledge
is an important direction open problems in language model research and could also be expected to
potentially improve multi-step reasoning abilities of language models. One potential method for
improving the quality of decoding could involve generating multiple reasoning paths and scoring
each of them with a veriﬁer, though this requires training the veriﬁer (Cobbe et al., 2021; Shen et al.,
2021; Thoppilan et al., 2022).
D.3 Additional Robustness Analysis
As the experiments in the main paper use a ﬁxed number of few-shot exemplars (8; as constrained by
the input length of 1024 tokens), we verify that the chain-of-thought prompting is robust to various
numbers of few-shot exemplars. We run experiments for LaMDA 137B, comparing chain-of-thought
prompting with standard prompting for the ﬁve datasets where standard prompting had a mostly ﬂat
scaling curve (the largest model did not achieve high performance). As shown in Figure 11, the
improvement of chain-of-thought prompting over standard prompting remains robust to varying the
number of few-shot exemplars in the prompt.
2812468051015 Solve rate (%)GSM8K
124680204060MultiArith
(MAWPS)
124680255075100
Number of few-shot exemplarsSports
UnderstandingStandard prompting
Chain of thought prompting
124680255075100Coin Flip
12340255075100Last Letter
Concatenation
Figure 11: The improvement of chain of thought prompting over standard prompting appears robust
to varying the number of few-shot exemplars in the prompt.
Table 12: Summary of math word problem benchmarks we use in this paper with examples. N:
number of evaluation examples.
Dataset N Example problem
GSM8K 1,319 Josh decides to try ﬂipping a house. He buys a house for $80,000 and then puts
in $50,000 in repairs. This increased the value of the house by 150%. How
much proﬁt did he make?
SV AMP 1,000 Each pack of dvds costs 76 dollars. If there is a discount of 25 dollars on each
pack. How much do you have to pay to buy each pack?
ASDiv 2,096 Ellen has six more balls than Marin. Marin has nine balls. How many balls does
Ellen have?
AQuA 254 A car is being driven, in a straight line and at a uniform speed, towards the base
of a vertical tower. The top of the tower is observed from the car and, in the
process, it takes 10 minutes for the angle of elevation to change from 45to 60.
After how much more time will this car reach the base of the tower? Answer
Choices: (a) 5p
3+ 1 (b) 6p
3+p
2(c) 7p
3- 1 (d) 8p
3- 2 (e) None of these
MAWPS: SingleOp 562 If there are 7 bottle caps in a box and Linda puts 7 more bottle caps inside, how
many bottle caps are in the box?
MAWPS: SingleEq 508 Benny bought a soft drink for 2 dollars and 5 candy bars. He spent a total of 27
dollars. How much did each candy bar cost?
MAWPS: AddSub 395 There were 6 roses in the vase. Mary cut some roses from her ﬂower garden.
There are now 16 roses in the vase. How many roses did she cut?
MAWPS: MultiArith 600 The school cafeteria ordered 42 red apples and 7 green apples for students
lunches. But, if only 9 students wanted fruit, how many extra did the cafeteria
end up with?
29E Additional Details
Version Control
V5!V6. Fixed minor typo in Figure 3.
V4!V5. Added Codex and UL2 results. Small changes to writing and style of paper.
V3!V4. Fixed typo in Figure 3 and added a couple citations.
V2!V3. Added GPT-3 results. Added SV AMP and AQuA eval datasets for math. Added SayCan
eval for commonsense. Added Extended Related Work section (Appendix C). Added ablations for
Commonsense and Symbolic Reasoning (Table 7). Added FAQ section (Appendix A). Added raw
results in Appendix B.
V1!V2. Added PaLM results (V1 only had LaMDA).
E.1 Reproducibility Statement
As our results make use of two sets of large language models that is not publicly available, we take
the following actions to facilitate reproducibility. First, we provide the exact input prompts for all
tasks in Table 20–Table 27 in Appendix G (and emphasize that we do not perform any ﬁnetuning and
only apply prompting to off-the-shelf language models). Second, we conduct experiments using the
publicly available GPT-3 API for four model scales text-ada-001, text-babbage-001, text-curie-001,
text-davinci-002). Finally, we make exact inputs, targets, and predictions for LaMDA 137B for each
task available as a zip ﬁle in the supplementary material.
E.2 Computational Resources
For all three language models we evaluated, we did prompting-based inference only. No ﬁnetuning
was done for this paper. For inference on LaMDA 137B we use TPU v3 (8x8 conﬁguration, 64 chips
/ 128 cores), and for inference on PaLM 540B we use TPU v4 (4x4x12 conﬁguration, 192 chips / 384
cores). GPT-3 experiments were done using the public API.5
E.3 Dataset Details and Licenses
We list the details and licenses for all arithmetic and commonsense datasets used in this paper. The
symbolic reasoning datasets were created synthetically, as described in Section 4.
Arithmetic reasoning
•Math Word Problem Repository (Koncel-Kedziorski et al., 2016): AddSub (Hosseini
et al., 2014): https://www.cs.washington.edu/nlp/arithmetic ; MultiArith (Roy
and Roth, 2015), license: CC BY 4.0.
• ASDiv (Miao et al., 2020): https://github.com/chaochun/nlu-asdiv-dataset .
•AQuA (Ling et al., 2017): https://github.com/deepmind/AQuA , license: https://
github.com/deepmind/AQuA/blob/master/LICENSE .
•GSM8K (Cobbe et al., 2021): https://github.com/openai/grade-school-math ,
MIT license: https://github.com/openai/grade-school-math/blob/master/
LICENSE .
•SV AMP (Patel et al., 2021): https://github.com/arkilpatel/SVAMP , MIT license:
https://github.com/arkilpatel/SVAMP/blob/main/LICENSE .
Commonsense reasoning
•CSQA (Talmor et al., 2019): https://www.tau-nlp.org/commonsenseqa ,https://
github.com/jonathanherzig/commonsenseqa .
5https://beta.openai.com/docs/api-reference/making-requests
30•StrategyQA (Geva et al., 2021): we use the open-domain setting (question-only set)
from BIG-bench collaboration (2021): https://github.com/google/BIG-bench/
tree/main/bigbench/benchmark_tasks/strategyqa . The original dataset is from
https://github.com/eladsegal/strategyqa , MIT license: https://github.com/
eladsegal/strategyqa/blob/main/LICENSE .
•Date understanding and sports understanding from BIG-Bench (BIG-bench collaboration,
2021): Apache License v.2: https://github.com/google/BIG-bench/blob/main/
LICENSE .
•SayCan (Ahn et al., 2022): SayCan dataset can be accessed at https://say-can.github.
io/under CC BY 4.0 license.
31F Appendix: Input/Output Examples
Table 13: Examples of correct and incorrect chains of thought produced by LaMDA 137B on the
letter concatenation task.
QUESTION :Take the last letters of the words in “Waldo Schmidt” and concatenate them.
MODEL ANSWER (CORRECT): The last letter of “Waldo” is “o”. The last letter of “Schmidt” is “t”.
Concatenating them is “ot”. So the answer is ot. X
QUESTION :Take the last letters of the words in “Daniel Friedman” and concatenate them.
MODEL ANSWER (INCORRECT): The last letter of “Daniel” is “l”. The last letter of “Friedman” is “m”.
Concatenating them is “lm”. So the answer is lm. 7
Table 14: Examples of correct and incorrect chains of thought produced by LaMDA 137B on the
coinﬂip task.
QUESTION :A coin is heads up. Andree ﬂips the coin. Audrie does not ﬂip the coin. Is the coin still heads up?
MODEL ANSWER (CORRECT): The coin was ﬂipped by Andree. So the coin was ﬂipped 1 time, which is
an odd number. The coin started heads up, so after an odd number of ﬂips, it will be tails up. So the answer is no.
X
QUESTION :A coin is heads up. Kristian does not ﬂip the coin. Dallas does not ﬂip the coin. Is the coin still
heads up?
MODEL ANSWER (INCORRECT): The coin was ﬂipped by Kristian. So the coin was ﬂipped 1 time, which
is an odd number. The coin started heads up, so after an odd number of ﬂips, it will be tails up. So the answer is
no.7
Table 15: Examples of correct and incorrect chains of thought produced by LaMDA 137B on CSQA.
QUESTION :When a person is beginning work, what are they building?
Answer Choices: (a) time (b) accomplishing (c) working (d) momentum (e) tiredness
MODEL ANSWER (CORRECT): The answer must be something that is built. Of the above choices, only
momentum is built. So the answer is (d). X
QUESTION :Where are you likely to ﬁnd a hamburger?
Answer Choices: (a) fast food restaurant (b) pizza (c) ground up dead cows (d) mouth (e) cow carcus
MODEL ANSWER (CORRECT): The answer must be a place where hamburgers are found. Of the above
choices, only fast food restaurants serve hamburgers. So the answer is (a). X
QUESTION :Aside from water and nourishment what does your dog need?
Answer Choices: (a) bone (b) charm (c) petted (d) lots of attention (e) walked
MODEL ANSWER (INCORRECT): The answer must be something that a dog needs. Of the above choices,
only bone is something that a dog needs. So the answer is (a). 7(Ground truth is (d).)
QUESTION :What are you waiting alongside with when you’re in a reception area?
Answer Choices: (a) motel (b) chair (c) hospital (d) people (e) hotels
MODEL ANSWER (INCORRECT): The answer must be something that is waiting with you in a reception
area. Of the above choices, only people are waiting with you in a reception area. So the answer is (e). 7(Ground
truth is (d).)
32Table 16: Examples of correct and incorrect chains of thought produced by LaMDA 137B on
StrategyQA.
QUESTION :Will The Exorcist stimulate limbic system?
MODEL ANSWER (CORRECT): The Exorcist is a horror movie. Horror movies are scary. The limbic system
is involved in fear. Thus, The Exorcist will stimulate the limbic system. So the answer is yes. X
QUESTION :Was Jackson Pollock trained by Leonardo da Vinci?
MODEL ANSWER (CORRECT): Leonardo da Vinci lived in the 15th century. Jackson Pollock lived in the
20th century. Thus, Jackson Pollock could not have been trained by Leonardo da Vinci. So the answer is no. X
QUESTION :Can Harry Potter book a ﬂight on Asiana Airlines?
MODEL ANSWER (INCORRECT): Harry Potter is a ﬁctional character. Thus, Harry Potter can do anything.
So the answer is yes. 7(Ground truth is “no”.)
QUESTION :Would a sophist use an épée?
MODEL ANSWER (INCORRECT): A sophist is a person who is skilled in the art of persuasion. An épée is
a type of sword. Thus, a sophist could use an épée. So the answer is yes. 7(Ground truth is “no”.)
Table 17: Examples of correct and incorrect chains of thought produced by LaMDA 137B on Date
Understanding.
QUESTION :May 6, 1992 is like yesterday to Jane, but that is actually ten years ago. What is the date a month
ago in MM/DD/YYYY?
MODEL ANSWER (CORRECT): May 6, 1992 is ten years ago, so today is May 6, 2002. So a month ago will
be April 6, 2002. So the answer is 04/06/2002. X
QUESTION :This is the last day of 1899. What is the date 24 hours later in MM/DD/YYYY?
MODEL ANSWER (CORRECT): Today is 12/31/1899. 24 hours later will be 01/01/1900. So the answer is
01/01/1900. X
QUESTION :Jane scheduled 3 apointments with 5 poeple for tomorrow (Tue, 7/9/1972). What is the date one
week ago from today in MM/DD/YYYY?
MODEL ANSWER (INCORRECT): Tomorrow is 7/9/1972. One week ago from today is 7/2/1972. So the
answer is 7/2/1972. 7(Ground truth is 07/01/1972.)
QUESTION :Today is the palindrome day of 2020, because the MMDDYYYY format of the date is the same
backwards as forwards. What is the date tomorrow in MM/DD/YYYY?
MODEL ANSWER (INCORRECT): Today is 02/29/2020. Tomorrow will be 03/01/2020. So the answer is
03/01/2020. 7(Ground truth is “02/03/2020”.)
Table 18: Examples of correct and incorrect chains of thought produced by LaMDA 137B on Sports
Understanding.
QUESTION :Is the following sentence plausible? “Joao Moutinho was out at third.”
MODEL ANSWER (CORRECT): Joao Moutinho is a soccer player. Being out at third is part of baseball, not
soccer. So the answer is no. X
QUESTION :Is the following sentence plausible? “Malcolm Brogdon eurostepped to the basket in the NBA
Championship.”
MODEL ANSWER (CORRECT): Malcolm Brogdon is a basketball player. Eurostepping to the basket is part
of basketball. So the answer is yes. X
QUESTION :Is the following sentence plausible? “Derrick White backhanded a shot.”
MODEL ANSWER (INCORRECT): Derrick White is a basketball player. Backhanding a shot is part of
basketball. So the answer is yes. 7(Ground truth is “no”, backhanding a shot is part of hockey.)
QUESTION :Is the following sentence plausible? “Nazem Kadri was out at home.”
MODEL ANSWER (INCORRECT): Nazem Kadri is a hockey player. Being out at home is part of hockey.
So the answer is yes. 7(Ground truth is “no”, being out at home is part of baseball.)
33Table 19: Examples of correct and incorrect chains of thought produced by PaLM 540B on SayCan
robot planning tasks.
QUESTION :I spilled my coke on the table, could you throw it away and then bring me something to help clean?
MODEL ANSWER (CORRECT): Explanation: The user has spilled their coke on the table. I will throw away
the coke and then bring the user a sponge. Plan: ﬁnd(coke), pick(coke), ﬁnd(trash), put(coke), ﬁnd(sponge),
pick(sponge), ﬁnd(table), put(sponge). X
QUESTION :Could you compost that apple for me?
MODEL ANSWER (CORRECT): Explanation: The user would like me to compost the apple. I will move the
apple to the compost. Plan: ﬁnd(apple), pick(apple), ﬁnd(trash), put(apple). X
QUESTION :Can you bring me a drink that is not coke?
MODEL ANSWER (INCORRECT): Explanation: The user asked me to bring a drink that is not coke, I will
bring a sprite. Plan: ﬁnd(sprite), pick(sprite), ﬁnd(user), put(sprite). 7(Feasible answers including ﬁnding lime
soda, pepsi, grapefruit soda or water, because there is no sprite in the scene as deﬁned in Table 28.)
QUESTION :Can you bring me something salty?
MODEL ANSWER (INCORRECT): Explanation: The user would like something salty. There are several
options in the objects list, so I will bring the user a lime soda. Plan: ﬁnd(lime soda), pick(lime soda), ﬁnd(user),
put(lime soda). 7(Feasible answers including ﬁnding chips. The model misidentify lime soda as salty.)
34G Appendix: Full Prompts
Table 20: Few-shot exemplars for full chain of thought prompt for math word problems. This set of
exemplars was used for all math word problem datasets except AQuA.
PROMPT FOR MATH WORD PROBLEMS
Q:There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there
will be 21 trees. How many trees did the grove workers plant today?
A:There are15trees originally. Then there were 21trees aftersome more were planted. Sothere must have
been 21-15=6. The answer is 6.
Q:If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?
A:There areoriginally 3cars. 2more carsarrive. 3+2=5. The answer is 5.
Q:Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?
A:Originally, Leah had32choco lates. Hersisterhad42.Sointotalthey had32+42=74.Aftereating35,they
had74-35=39. The answer is 39.
Q:Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did
Jason give to Denny?
A:Jasonstarted with 20lollipops. Then hehad12aftergivingsome toDenny. Sohegave Denny 20-12=8.
The answer is 8.
Q:Shawn has ﬁve toys. For Christmas, he got two toys each from his mom and dad. How many toys does he
have now?
A:Shawn started with 5toys. Ifhegot2toys each from hismom anddad, then thatis4more toys. 5+4=9.
The answer is 9.
Q:There were nine computers in the server room. Five more computers were installed each day, from monday
to thursday. How many computers are now in the server room?
A:There were originally 9computers.Foreach of4days, 5more computerswere added. So5*4=20
computerswere added. 9+20is29. The answer is 29.
Q:Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf
balls did he have at the end of wednesday?
A:Michael started with 58golfballs. Afterlosing23ontuesday, hehad58-23=35.Afterlosing2more, he
had35-2=33golfballs. The answer is 33.
Q:Olivia has $23. She bought ﬁve bagels for $3 each. How much money does she have left?
A:Olivia had23dollars. 5bagels for3dollarseach willbe5x3=15dollars. Soshehas23-15dollarsleft.23
-15is8. The answer is 8.
35Table 21: Few-shot exemplars for full chain of thought prompt for AQuA.
PROMPT FOR AQ UAALGEBRAIC WORD PROBLEMS
Q:John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers
is?
Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64
A:If10isadded toeach number,then themean ofthenumbers alsoincreases by10.Sothenewmean would be
50. The answer is (a).
Q:If a / b = 3/4 and 8a + 5b = 22,then ﬁnd the value of a.
Answer Choices: (a) 1/2 (b) 3/2 (c) 5/2 (d) 4/2 (e) 7/2
A:Ifa/b=3/4,then b=4a/3.So8a+5(4a /3)=22.This simpliﬁesto8a+20a/3=22,which means 44a/3
=22.Soaisequal to3/2. The answer is (b).
Q:A person is traveling at 20 km/hr and reached his destiny in 2.5 hr then ﬁnd the distance?
Answer Choices: (a) 53 km (b) 55 km (c) 52 km (d) 60 km (e) 50 km
A:Thedistance thatthepersontraveled would have been 20km/hr *2.5hrs=50km. The answer is (e).
Q:How many keystrokes are needed to type the numbers from 1 to 500?
Answer Choices: (a) 1156 (b) 1392 (c) 1480 (d) 1562 (e) 1788
A:There are9one-digit numbers from 1to9.There are90two-digit numbers from 10to99.There are401
three -digit numbers from 100to500. 9+90(2) +401(3) =1392. The answer is (b).
Table 22: Few-shot exemplars for full chain of thought prompt for the last letter concatenation task.
PROMPT FOR LAST LETTER CONCATENATION
Q:Take the last letters of the words in "Elon Musk" and concatenate them.
A:Thelastletterof"Elon" is"n".Thelastletterof"Musk" is"k".Concatenatingthem is"nk". The answer is nk.
Q:Take the last letters of the words in "Larry Page" and concatenate them.
A:Thelastletterof"Larry" is"y".Thelastletterof"Page" is"e".Concatenatingthem is"ye". The answer is ye.
Q:Take the last letters of the words in "Sergey Brin" and concatenate them.
A:Thelastletterof"Sergey" is"y".Thelastletterof"Brin" is"n".Concatenatingthem is"yn". The answer is
yn.
Q:Take the last letters of the words in "Bill Gates" and concatenate them.
A:Thelastletterof"Bill" is"l".Thelastletterof"Gates" is"s".Concatenatingthem is"ls". The answer is ls.
36Table 23: Few-shot exemplars for full chain of thought prompt for the coinﬂip task.
PROMPT FOR COIN FLIP
Q:Q: A coin is heads up. Ka ﬂips the coin. Sherrie ﬂips the coin. Is the coin still heads up?
A:Thecoin wasﬂipped byKaandSherrie.Sothecoin wasﬂipped 2times, which isaneven number.Thecoin
started heads up,soafteraneven numberofﬂips, itwillstillbeheads up. So the answer is yes.
Q:A coin is heads up. Jamey ﬂips the coin. Teressa ﬂips the coin. Is the coin still heads up?
A:Thecoin wasﬂipped byJamey andTeressa. Sothecoin wasﬂipped 2times, which isaneven number.The
coin started heads up,soafteraneven numberofﬂips, itwillstillbeheads up. So the answer is yes.
Q:A coin is heads up. Maybelle ﬂips the coin. Shalonda does not ﬂip the coin. Is the coin still heads up?
A:Thecoin wasﬂipped byMaybelle. Sothecoin wasﬂipped 1time, which isanoddnumber.Thecoin started
heads up,soafteranoddnumberofﬂips, itwillbetails up. So the answer is no.
Q:A coin is heads up. Millicent does not ﬂip the coin. Conception ﬂips the coin. Is the coin still heads up?
A:Thecoin wasﬂipped byConception. Sothecoin wasﬂipped 1time, which isanoddnumber.Thecoin
started heads up,soafteranoddnumberofﬂips, itwillbetails up. So the answer is no.
Q:A coin is heads up. Sal ﬂips the coin. Raymond does not ﬂip the coin. Is the coin still heads up?
A:Thecoin wasﬂipped bySal.Sothecoin wasﬂipped 1time, which isanoddnumber.Thecoin started heads
up,soafteranoddnumberofﬂips, itwillbetails up. So the answer is no.
Q:A coin is heads up. Conception ﬂips the coin. Kristian does not ﬂip the coin. Is the coin still heads up?
A:Thecoin wasﬂipped byConception. Sothecoin wasﬂipped 1time, which isanoddnumber.Thecoin
started heads up,soafteranoddnumberofﬂips, itwillbetails up. So the answer is no.
Q:A coin is heads up. Inga does not ﬂip the coin. Elanor does not ﬂip the coin. Is the coin still heads up?
A:Thecoin wasﬂipped bynoone. Sothecoin wasﬂipped 0times. Thecoin started heads up,anditwasnot
ﬂipped, soitisstillheads up. So the answer is yes.
Q:A coin is heads up. Ryan ﬂips the coin. Shaunda ﬂips the coin. Is the coin still heads up?
A:Thecoin wasﬂipped byRyan andShaunda. Sothecoin wasﬂipped 2times, which isaneven number.The
coin started heads up,soafteraneven numberofﬂips, itwillstillbeheads up. So the answer is yes.
37Table 24: Few-shot exemplars for full chain of thought prompt for CSQA. There are newlines
between the answer choices that are omitted in the table for space reasons.
PROMPT FOR CSQA
Q:What do people use to absorb extra ink from a fountain pen? Answer Choices: (a) shirt pocket (b)
calligrapher’s hand (c) inkwell (d) desk drawer (e) blotter
A:Theanswer must beanitem thatcanabsorb ink.Oftheabove choices, only blottersareused toabsorb ink.
So the answer is (e).
Q:What home entertainment equipment requires cable?
Answer Choices: (a) radio shack (b) substation (c) television (d) cabinet
A:Theanswer must require cable.Oftheabove choices, only television requires cable. So the answer is (c).
Q:The fox walked from the city into the forest, what was it looking for? Answer Choices: (a) pretty ﬂowers (b)
hen house (c) natural habitat (d) storybook
A:Theanswer must besome thing intheforest.Oftheabove choices, only naturalhabitatisintheforest. So the
answer is (b).
Q:Sammy wanted to go to where the people were. Where might he go? Answer Choices: (a) populated areas
(b) race track (c) desert (d) apartment (e) roadblock
A:Theanswer must beaplace with alotofpeople.Oftheabove choices, only populated areashave alotof
people. So the answer is (a).
Q:Where do you put your grapes just before checking out? Answer Choices: (a) mouth (b) grocery cart (c)super
market (d) fruit basket (e) fruit market
A:Theanswer should betheplace where grocery items areplaced beforecheck ingout.Oftheabove choices,
grocery cartmakes themost sense forholdinggrocery items. So the answer is (b).
Q:Google Maps and other highway and street GPS services have replaced what? Answer Choices: (a) united
states (b) mexico (c) countryside (d) atlas
A:Theanswer must besome thing thatused todowhat Google Maps andGPS services do,which istogive
directions. Oftheabove choices, only atlases areused togive directions. So the answer is (d).
Q:Before getting a divorce, what did the wife feel who was doing all the work? Answer Choices: (a) harder (b)
anguish (c) bitterness (d) tears (e) sadness
A:Theanswer should bethefeelingofsome onegettingdivorced who wasdoingallthework. Oftheabove
choices, theclosestfeelingisbitterness. So the answer is (c).
38Table 25: Few-shot exemplars for full chain of thought prompt for StrategyQA.
PROMPT FOR STRATEGY QA
Q:Do hamsters provide food for any animals?
A:Ham sters areprey animals. Prey arefood forpreda tors. Thus, hamsters provide food forsome animals. So
the answer is yes.
Q:Could Brooke Shields succeed at University of Pennsylvania?
A:Brooke Shields went toPrince tonUniversity. Prince tonUniversityisabout asacademically rigorousasthe
UniversityofPenn sylvania.Thus, Brooke Shields could alsosucceed attheUniversityofPenn sylvania. So the
answer is yes.
Q:Yes or no: Hydrogen’s atomic number squared exceeds number of Spice Girls?
A:Hydrogenhasanatomic numberof1.1squared is1.There are5Spice Girls. Thus, Hydrogen’s atomic
numbersquared islessthan 5. So the answer is no.
Q:Yes or no: Is it common to see frost during some college commencements?
A:College commence ment ceremonies canhappeninDecember,May, andJune. Decemberisinthewinter,so
there canbefrost. Thus, there could befrost atsome commence ments. So the answer is yes.
Q:Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?
A:TheWar inVietnam was6months. Thegestationperiod forallama is11months, which ismore than 6
months. Thus, allama could notgive birth twice duringtheWar inVietnam. So the answer is no.
Q:Yes or no: Would a pear sink in water?
A:Thedensityofapear isabout 0:6g=cm3,which islessthan water.Objects lessdense than waterﬂoat. Thus,
apear would ﬂoat. So the answer is no.
Table 26: Few-shot exemplars for full chain of thought prompt for Date Understanding.
PROMPT FOR DATE UNDERSTANDING
Q:2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?
A:If2015 iscomingin36hours, then itiscomingin2days. 2days before01/01/2015 is12/30/2014, sotoday
is12/30/2014. Sooneweek from todaywillbe01/05/2015. So the answer is 01/05/2015.
Q:The ﬁrst day of 2019 is a Tuesday, and today is the ﬁrst Monday of 2019. What is the date today in
MM/DD/YYYY?
A:Iftheﬁrstdayof2019 wasTues day,then 01/01/2019 wasaTues day.Todayistheﬁrstmonday,would besix
days later. Sotodayis01/07/2019. So the answer is 01/07/2019.
Q:The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10
days ago in MM/DD/YYYY?
A:One dayafter06/01/1943 is06/02/1943, sotodayis06/02/1943. 10days beforetodayis05/23/1943. So the
answer is 05/23/1943.
Q:It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY?
A:Todayis04/19/1969. 24hours later isonedayaftertoday, which would be04/20/1969. So the answer is
04/20/1969.
Q:Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours
later in MM/DD/YYYY?
A:Todayis03/12/2002. Sothedate 24hours later willbe03/13/2002. So the answer is 03/13/2002.
Q:Jane was born on the last day of Feburary in 2001. Today is her 16-year-old birthday. What is the date
yesterday in MM/DD/YYYY?
A:ThelastdayofFebru aryisthe28th, soJane wasborn on02/28/2001. Todayisher16-year oldbirth day, so
todayis02/28/2017. Soyesterdaywas02/27/2017. So the answer is 02/27/2017.
39Table 27: Few-shot exemplars for full chain of thought prompt for Sports Understanding.
PROMPT FOR SPORTS UNDERSTANDING
Q:Is the following sentence plausible? “Kyle Palmieri was called for slashing.”
A:Kyle Palmieri isahockey player. Beingcalled forslash ingispartofhockey. So the answer is yes.
Q:Is the following sentence plausible? “Joao Moutinho caught the screen pass in the NFC championship.”
A:Joao Moutinho isasoccerplayer. TheNFC cham pionship ispartofAmer icanfootball, notsoccer. So the
answer is no.
Q:Is the following sentence plausible? “Carson Wentz set the pick and roll.”
A:CarsonWentz isanAmer icanfootballplayer. Pick androllispartofbasketball, notfootball. So the answer
is no.
Q:Is the following sentence plausible? “Jonas Valanciunas beat the buzzer.”
A:Jonas Valan ciunasisabasketballplayer. Beatingthebuzzer ispartofbasketball. So the answer is yes.
Q:Is the following sentence plausible? “Jamel Murray was perfect from the line.”
A:JamalMurrayisabasketballplayer. Beingperfectfrom thelineispartofbasketball. So the answer is yes.
Q:Is the following sentence plausible? “Sam Darnold passed the puck.”
A:Sam Darnold isaAmer icanfootballplayer. Passingthepuck ispartofhockey, notAmer icanfootball. So the
answer is no.
Q:Is the following sentence plausible? “Draymond Green threw a touchdown.”
A:Dray mond Green isanbasketballplayer. Throw ingatouch down ispartoffootball, notbasketball. So the
answer is no.
Q:Is the following sentence plausible? “Malcolm Brogdon banked the shot in.”
A:Malcolm Brog donisabasketballplayer. Bank ingtheshot inispartofbasketball. So the answer is yes.
40Table 28: Few-shot exemplars for full chain of thought prompt for SayCan robot planning tasks.
PROMPT FOR SAYCANROBOT PLANNING
Locations = [counter, table, user, trash, bowl].
Objects = [7up, apple, kettle chips, tea, multigrain chips, coke, lime soda, jalapeno chips, rice chips, orange,
grapefruit soda, pepsi, redbull, energy bar, sponge, water].
The robot can pick up items with pick(object) and put down items with put(object) as well as ﬁnd objects or
locations with ﬁnd(). The robot can only understand the explicit locations and objects listed.
Human : I’m hungry, can you bring me some chips.
Explanation :Theuser ishungryandhasasked forchips. There areseveraltypes ofchips avail able, Iwillbring
theuser thekettlechips.
Plan : 1. ﬁnd(kettle chips), 2. pick(kettle chips), 3. ﬁnd(user), 4. put(kettle chips), 5. done().
Human : How would you move the grapefruit drink from the table to the counter?
Explanation :Theuser hasasked metomove thegrape fruit drink tothecounter.
Plan : 1. ﬁnd(grapefruit soda), 2. pick(grapefruit soda), 3. ﬁnd(counter), 4. put(grapefruit soda), 5. done().
Human : How would you bring me some snacks?
Explanation :Theuser hasasked forsnacks, Iwillchoose twoitems andbring them. Iwillbring jalapeno chips
andanapple.
Plan : 1. ﬁnd(jalapeno chips), 2. pick(jalapeno chips), 3. ﬁnd(user), 4. put(jalapeno chips), 5. ﬁnd(apple), 6.
pick(apple), 7. ﬁnd(user), 8. put(apple), 9. done().
Human : How would you bring me something to eat that isn’t a fruit?
Explanation :Theuser hasasked forafood thatisn’t afruit, Iwillbring anenergy bartothem.
Plan : 1. ﬁnd(energy bar), 2. pick(energy bar), 3. ﬁnd(user), 4. put(energy bar), 5. done().
Human : How would you put the rice chips in the bowl and then move the tea to the table?
Explanation :Theuser hasasked metodotwotasks, Iwilldooneandthen theother.
Plan : 1. ﬁnd(rice chips), 2. pick(rice chips), 3. ﬁnd(bowl), 4. put(rice chips), 5. ﬁnd(tea), 6. pick(tea), 7.
ﬁnd(table), 8. put(tea), 9. done().
Human : How would you throw away a redbull?
Explanation :Theuser hasasked metothrow away theredbull, Iwillmove ittothetrash.
Plan : 1. ﬁnd(redbull), 2. pick(redbull), 3. ﬁnd(trash), 4. put(redbull), 5. done().
Human : Bring me a drink.
Explanation :Theuser hasasked foradrink andthere aremany options. Iwillbring them awater.
Plan : 1. ﬁnd(water), 2. pick(water), 3. ﬁnd(user), 4. put(water), 5. done().
41H Appendix: Alternate Annotators for MWP
Table 29: Few-shot exemplars for full chain of thought prompt for math word problems. These
exemplars are the same as in Table 20, except that the chains of thought were written by a different
annotator (“Annotator B” instead of “Annotator A”). Annotators were co-authors and familiar with
the goal of chain of thought prompting.
PROMPT FOR MATH WORD PROBLEMS
Q:There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there
will be 21 trees. How many trees did the grove workers plant today?
A:There are21trees now andthere are15trees inthebeginning, sothework ersplant 21-15=6trees. The
answer is 6.
Q:If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?
A:There are3carsinthebeginning, 2more arrive, sonow there should be3+2=5cars. The answer is 5.
Q:Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?
A:Leah had32choco lates andhersisterhad42,intotalthey have 32+42=74choco lates. Afterthey ate35,
now there are74-35=39choco lates. The answer is 39.
Q:Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did
Jason give to Denny?
A:Jasonstarted with 20lollipops, butnow heonly has12,sohegave Denny 20-12=8lollipops. The answer
is 8.
Q:Shawn has ﬁve toys. For Christmas, he got two toys each from his mom and dad. How many toys does he
have now?
A:Shawn got2toys each from hismom anddad, sohegot2*2=4more, now hewillhave 5+4=9toys. The
answer is 9.
Q:There were nine computers in the server room. Five more computers were installed each day, from monday
to thursday. How many computers are now in the server room?
A:5computerswere installed from mondaytothurs day, sointotal5*4=20computersareinstalled. 9
computersarethere inthebeginning, sonow there are20+9=29computers. The answer is 29.
Q:Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf
balls did he have at the end of wednesday?
A:Michael started with 58golfballs andlost23,sohehas58-23=35.Afterhelost2more, hehas35-2=33
balls now. The answer is 33.
Q:Olivia has $23. She bought ﬁve bagels for $3 each. How much money does she have left?
A:5bagels for$3each should cost5*3=15dollars. Olivia had$23inthebeginning, sonow shehas23-15=
8dollarsleft. The answer is 8.
42Table 30: Few-shot exemplars for full chain of thought prompt for math word problems. These
exemplars are the same as in Table 20, except that the chains of thought were written by a different
annotator (“Annotator C” instead of “Annotator A”).
PROMPT FOR MATH WORD PROBLEMS
Q:There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there
will be 21 trees. How many trees did the grove workers plant today?
A:Westart with 15trees. Later wehave 21trees. Thedifference must bethenumberoftrees they planted. So,
they must have planted 21-15=6trees. The answer is 6.
Q:If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?
A:There are3carsintheparkinglotalready. 2more arrive. Now there are3+2=5cars. The answer is 5.
Q:Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?
A:Leah had32choco lates andLeah’s sisterhad42.That means there were originally 32+42=74choco lates.
35have been eaten. Sointotalthey stillhave 74-35=39choco lates. The answer is 39.
Q:Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did
Jason give to Denny?
A:Jasonhad20lollipops. Since heonly has12now, hemust have given theresttoDenny. Thenumberof
lollipops hehasgiven toDenny must have been 20-12=8lollipops. The answer is 8.
Q:Shawn has ﬁve toys. For Christmas, he got two toys each from his mom and dad. How many toys does he
have now?
A:Hehas5toys. Hegot2from mom, soafterthathehas5+2=7toys. Then hegot2more from dad, soin
totalhehas7+2=9toys. The answer is 9.
Q:There were nine computers in the server room. Five more computers were installed each day, from monday
to thursday. How many computers are now in the server room?
A:There are4days from mondaytothurs day.5computerswere added each day.That means intotal4*5=20
computerswere added. There were 9computersinthebeginning, sonow there are9+20=29computers. The
answer is 29.
Q:Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf
balls did he have at the end of wednesday?
A:Michael initially had58balls. Helost23onTues day, soafterthathehas58-23=35balls. OnWednes day
helost2more sonow hehas35-2=33balls. The answer is 33.
Q:Olivia has $23. She bought ﬁve bagels for $3 each. How much money does she have left?
A:Shebought 5bagels for$3each. This means shespent 5*$3=$15onthebagels. Shehad$23inbeginning,
sonow shehas$23-$15=$8. The answer is 8.
43